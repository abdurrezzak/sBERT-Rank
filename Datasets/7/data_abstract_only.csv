,id,abstract,keys,#tokens,#keys
0,1,The main result of this paper is an explicit disperser for two independent sources on n bits each of entropy k = n o(1 . Put differently setting N = 2 n and K = 2 k  we construct explicit N × N Boolean matrices for which no K × K sub-matrix is monochromatic. Viewed as adjacency matrices of bipartite graphs this gives an explicit construction of K-Ramsey bipartite graphs of size N . This greatly improves the previous bound of k = o(n of Barak Kindler Shaltiel Sudakov and Wigderson 4]. It also significantly improves the 25-year record of k =  O(n on the special case of Ramsey graphs due to Frankl and Wilson 9]. The construction uses besides classical extractor ideas almost all of the machinery developed in the last couple of years for extraction from independent sources including · Bourgain's extractor for 2 independent sources of some entropy rate lt 1/2 5 · Raz's extractor for 2 independent sources one of which has any entropy rate gt 1/2 18 Supported by a Princeton University startup grant. Most of this work was done while the author was visiting Princeton University and the Institute for Advanced Study. Supported in part by an MCD fellowship from UT Austin and NSF Grant CCR-0310960. This research was supported by the United States-Israel Binational Science Foundation BSF grant 2004329. §This research was supported by NSF Grant CCR-0324906. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and/or a fee. STOC'06 May 21­23 2006 Seattle Washington USA. Copyright 2006 ACM 1-59593-134-1/06/0005 ...  5.00. · Rao's extractor for 2 independent block-sources of entropy n 1 17 · The Challenge-Response mechanism for detecting entropy concentration of 4]. The main novelty comes in a bootstrap procedure which allows the Challenge-Response mechanism of 4 to be used with sources of less and less entropy using recursive calls to itself. Subtleties arise since the success of this mechanism depends on restricting the given sources and so recursion constantly changes the original sources. These are resolved via a new construct in between a disperser and an extractor which behaves like an extractor on sufficiently large subsources of the given ones. This version is only an extended abstract please see the full version available on the authors homepages for more details.,"['tools', 'Theorem', 'construction of disperser', 'disperser', 'independent source', 'randomness extraction', 'termination', 'extractors', 'polynomial time computable disperser', 'structure', 'distribution', 'Ramsey Graphs', 'explicit disperser', 'deficiency', 'block-sources', 'Extractors', 'entropy', 'Independent Sources', 'Ramsey graph', 'sum-product theorem', 'subsource somewhere extractor', 'bipartite graph', 'subsource', 'recursion', 'Dispersers', 'independent sources', 'Ramsey graphs', 'algorithms', 'resiliency', 'extractor']",439,30
1,2,This paper shows the importance that management plays in the protection of information and in the planning to handle a security breach when a theft of information happens. Recent thefts of information that have hit major companies have caused concern. These thefts were caused by companies inability to determine risks associated with the protection of their data and these companies lack of planning to properly manage a security breach when it occurs. It is becoming necessary if not mandatory for organizations to perform ongoing risk analysis to protect their systems. Organizations need to realize that the theft of information is a management issue as well as a technology one and that these recent security breaches were mainly caused by business decisions by management and not a lack of technology.,"['privacy', 'security breach', 'encryption', 'risk analysis', 'business practises and policy', 'information system', 'incident response plan', 'management issue', 'information security', 'data protection procedure', 'Information Security Management', 'data protection', 'cases of information theft', 'cyber crime', 'Security Management', 'human factor', 'confidential information', 'personal information', 'Information Security', 'theft of information']",129,20
2,3,We present a foundation for a computational meta-theory of languages with bindings implemented in a computer-aided formal reasoning environment. Our theory provides the ability to reason abstractly about operators languages open-ended languages classes of languages etc. The theory is based on the ideas of higher-order abstract syntax with an appropriate induction principle parameterized over the language i.e. a set of operators being used. In our approach  both the bound and free variables are treated uniformly and this uniform treatment extends naturally to variable-length bindings . The implementation is reflective namely there is a natural mapping between the meta-language of the theorem-prover and the object language of our theory. The object language substitution operation is mapped to the meta-language substitution and does not need to be defined recursively. Our approach does not require designing a custom type theory in this paper we describe the implementation of this foundational theory within a general-purpose type theory. This work is fully implemented in the MetaPRL theorem prover using the pre-existing NuPRL-like Martin-L¨of-style computational type theory. Based on this implementation we lay out an outline for a framework for programming language experimentation and exploration as well as a general reflective reasoning framework. This paper also includes a short survey of the existing approaches to syntactic reflection.,"['Reflection', 'Substitution', 'Higher-Order Abstract Syntax', 'Runtime code generation', 'Theory of syntax', 'system reflection', 'Type Theory', 'HOAS-style operations', 'programming language', 'Meta-syntax', 'NuPRL', 'Meta-reasoning', 'Recursive definition', 'uniform reflection framework', 'Reflective reasoning', 'higher-order abstract syntax', 'formal languages', 'Uniform reflection framework', 'MetaPRL', 'High order abstract syntax', 'Bruijn-style operations', 'Languages with Bindings', 'Programming Language Experimentation', 'formal definition and theory', 'Theorem prover', 'MetaPRL theorem prover', 'Meta-language', 'NuPRL-like Martin-Lof-style computational type theory', 'Languages with bindings', 'computer aided reasoning']",211,30
3,4,Database security has paramount importance in industrial civilian and government domains. Despite its importance our search reveals that only a small number of database security courses are being offered. In this paper we share our experience in developing and offering an undergraduate elective course on database security with limited resources. We believe that database security should be considered in its entirety rather than being component specific. Therefore  we emphasize that students develop and implement a database security plan for a typical real world application . In addition to the key theoretical concepts students obtain hands-on experience with two popular database systems . We encourage students to learn independently making use of the documentation and technical resources freely available on the Internet. This way our hope is that they will be able to adapt to emerging systems and application scenarios.,"['Laboratory/Active Learning', 'Access Privilege System', 'Database Security Education', 'Database system', 'Database security plan', 'Statistical Database Security', 'Database Security Plan', 'Hands-on experience', 'Assignments', 'Undergraduate students', 'Topics', 'Real Life Database Security', 'Undergraduate course', 'MySQL Security', 'Hands-on', 'Data access', 'Statistical Security', 'High Risk of Sensitive Information', 'Oracle Security', 'Importance of Database Security', 'Security breaches', 'Right Blend of Theory and Practice', 'Database Privacy', 'Cryptography', 'Database security course', 'Administrators', 'Secure information', 'Database Security Course', 'Few Database Security Courses', 'Database security', 'Undergraduate Database Security Course', 'Security Plan', 'Privacy Issues', 'XML Security', 'Real Life Databases Hands-on', 'Database Security', 'Labs']",139,37
4,5,"Emerging technologies are set to provide further provisions for computing in times when the limits of current technology of microelectronics become an ever closer presence. A technology roadmap document lists biologically-inspired computing and quantum computing as two emerging technology vectors for novel computing architectures A href=""5.html#12"">[43]. But the potential benefits that will come from entering the nanoelectronics era and from exploring novel nanotechnologies are foreseen to come at the cost of increased sensitivity to influences from the surrounding environment. This paper elaborates on a dependability perspective over these two emerging technology vectors from a designer's standpoint. Maintaining or increasing the dependability of unconventional computational processes is discussed in two different contexts one of a bio-inspired computing architecture the Embryonics project and another of a quantum computational architecture the QUERIST project).","['bio-inspired digital design', 'Computing architecture', 'Soft errors', 'Failure rate', 'Fault tolerance', 'QUERIST', 'Bio-computing', 'Dependability', 'fault-tolerance assessment', 'Digital devices', 'Reliability', 'environment', 'Emerging technologies', 'System design', 'Nano computing', 'emerging technologies', 'Embryonics', 'reliability', 'Self repair', 'Computer system', 'Error detection', 'Correction techniques', 'Computing technology', 'Self replication', 'Dependable system', 'Bio-inspired computing', 'Nanoelectronics', 'bio-inspired computing', 'quantum computing', 'Computing system', 'Quantum computing', 'evolvable hardware']",130,32
5,6,We present Repo-3D a general-purpose object-oriented library for developing distributed interactive 3D graphics applications across a range of heterogeneous workstations. Repo-3D is designed to make it easy for programmers to rapidly build prototypes using a familiar multi-threaded object-oriented programming paradigm. All data sharing of both graphical and non-graphical data is done via general-purpose remote and replicated objects presenting the illusion of a single distributed shared memory. Graphical objects are directly distributed circumventing the duplicate database problem and allowing programmers to focus on the application details. Repo-3D is embedded in Repo an interpreted lexically-scoped distributed programming language allowing entire applications to be rapidly prototyped. We discuss Repo-3D's design and introduce the notion of local variations to the graphical objects which allow local changes to be applied to shared graphical structures. Local variations are needed to support transient local changes such as highlighting and responsive local editing operations. Finally we discuss how our approach could be applied using other programming languages such as Java.,"['Distributed applications', 'local variations', 'Distributed processes', 'Prototyping', 'prototype', 'Programming language', 'programming language', 'Client-Server', 'Repo-3D', '3D Graphics', 'Multi-user interaction', 'distributed virtual environments', 'Shared memory', 'data distribution', 'Syncronisation', 'Object representation', '3D graphics library', 'Client-server approach', 'Local variation', 'distributed shared memory', '3D graphics application', 'shared-data object model', 'Object oriented', 'Heterogeneous workstation', 'Library', 'Data structures', 'Callbacks', 'object-oriented graphics', 'Graphical objects', 'Programming', 'multi-threaded programming', 'Interactive graphical application', 'Java', 'Data sharing', 'Object-oriented', 'duplicate database', 'Distributed graphics', 'Distributed systems', 'Properties', 'Replicated object', 'Change notification', 'object-oriented library', '3D graphics']",162,43
6,7,mechanisms and algorithms necessary to set up and maintain them. The operation of a scatternet requires some Bluetooth units to be inter-piconet units gateways which need to time-division multiplex their presence among their piconets. This requires a scatternet-scheduling algorithm that can schedule the presence of these units in an efficient manner. In this paper we propose a distributed scatternet-scheduling scheme that is implemented using the HOLD mode of Bluetooth and adapts to non-uniform and changing traffic. Another attribute of the scheme is that it results in fair allocation of bandwidth to each Bluetooth unit. This scheme provides an integrated solution for both intra-and inter-piconet scheduling i.e. for polling of slaves and scheduling of gateways.,"['Information exchange', 'Scatternets', 'Traffic Dependent Scheduling', 'Allocation of bandwidth', 'polling algorithm', 'Scheduling algorithm', 'HOLD mode', 'Piconet', 'Round Robin', 'scheduling', 'Time-division multiplex', 'traffic rate', 'Scatternet presence fraction', 'Non-uniform traffic', 'Slaves', 'gateway', 'Gateway slave traffic', 'traffic adaptive', 'virtual slave', 'Slave unit', 'Scatternet', 'Bluetooth', 'Distributed algorithm', 'Blueooth', 'scatternets', 'scheduling scheme', 'slave', 'Bluetooth technology', 'Fairness', 'Efficiency', 'Gateway', 'Rendezvous Points', 'changing traffic', 'Round Robin polling', 'scatternet', 'piconet', 'allocation of bandwidth', 'Master unit', 'Rendezvous point', 'bandwidth utilization', 'Scheduling of gateways', 'fairness', 'Non-gateway slave traffic', 'fair share', 'heuristic', 'Fair share', 'Piconet presence fraction', 'Rendezvous Point', 'master']",114,49
7,8,In this paper we present a simple but general 3D slicer for voxelizing polygonal model. Instead of voxelizing a model by projecting and rasterizing triangles with clipping planes the distance field is used for more accurate and stable voxelization. Distance transform is used with triangles on voxels of each slice. A voxel is marked with opacity only when the shortest distance between it and triangles is judged as intersection. With advanced programmable graphics hardware assistance surface and solid voxelization are feasible and more efficient than on a CPU.,"['GPU', 'object representation', 'Computational Geometry', 'Voxelization', 'Rendering cost', 'pixel shader', 'Polygonal model', 'triangles', 'Modeling', 'hausdorff distance', 'polygonal objects', 'volumetric representation', 'Object representation', 'infinitude', 'rendering', 'opacity', 'Surface voxelization', 'distance transform', 'Rendering', 'slice-based approach', 'geometric representation', 'Surface representation', 'adaptive dense voxelization', 'voxelization', 'Volumetric representation', 'Graphics hardware', 'Hausdorff distance', 'distance field', 'local distance field', 'Hausforff distance', 'GPU-based 3D slicer approach', '3D modelling', 'rasterization', 'Distance field', 'Rendering pipeline', 'Computer Graphics', '3D slicer', 'GPU computation', 'polygonal model', 'volume construction', 'Polygonal object']",88,41
8,9,This paper presents a CORBA-compliant middleware architecture that is more flexible and extensible compared to standard CORBA. The portable design of this architecture is easily integrated in any standard CORBA middleware for this purpose mainly the handling of object references IORs has to be changed. To encapsulate those changes we introduce the concept of a generic reference manager with portable profile managers. Profile managers are pluggable and in extreme can be downloaded on demand. To illustrate the use of this approach we present a profile manager implementation for fragmented objects and another one for bridging CORBA to the Jini world. The first profile manager supports truly distributed objects which allow seamless integration of partitioning  scalability fault tolerance end-to-end quality of service and many more implementation aspects into a distributed object without losing distribution and location transparency. The second profile manager illustrates how our architecture enables fully transparent access from CORBA applications to services on non-CORBA platforms,"['Flexible and extensible object middleware', 'Fault tolerant CORBA', 'Profile manager', 'extensibility', 'Middleware systems', 'Middleware', 'profile manager', 'IIOP', 'Object references', 'extensible and reconfigurable middleware', 'Middleware architecture', 'encapsulation', 'middleware architecture', 'Extensibility', 'Flexibility', 'Interoperability', 'integration', 'object oriented', 'implementation', 'IOR', 'Middleware platform', 'Ubiquitous computing', 'Distiributed applications', 'Extensions', 'Software architecture for middleware', 'Remote object', 'CORBA', 'middleware interoperability', 'Reference manager', 'distributed objects']",156,30
9,10,This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency idf . We show that an intuitive idf based probability function for the probability of a term being informative assumes disjoint document events. By assuming documents to be independent rather than disjoint we arrive at a Poisson-based probability of being informative. The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.,"['Noise probability', 'Information theory', 'information search', 'Assumptions', 'Poisson-based probability of being informative', 'inverse document frequency (idf)', 'Frequency-based probability', 'Collection space', 'Poisson based probability', 'relevance-based ranking of retrieved objects', 'document retrieval', 'Document frequency', 'Independent documents', 'entropy', 'inverse document frequency', 'probabilistic retrieval models', 'independence assumption', 'Document space', 'Term frequency', 'frequency-based term noise probability', 'Probabilistic information retrieval', 'computer science', 'independent and disjoint documents', 'Probability of being informative', 'probability theories', 'maximal informative signal', 'information theory', 'Normalisation', 'Inverse document frequency', 'Poisson distribution', 'Disjoint documents']",72,31
10,11,We bridge the gap between functional evaluators and abstract machines for the calculus using closure conversion transformation into continuation-passing style and defunctionalization. We illustrate this approach by deriving Krivine's abstract machine from an ordinary call-by-name evaluator and by deriving an ordinary call-by-value evaluator from Felleisen et al.'s CEK machine. The first derivation is strikingly simpler than what can be found in the literature. The second one is new. Together they show that Krivine's abstract machine and the CEK machine correspond to the call-by-name and call-by-value facets of an ordinary evaluator for the calculus. We then reveal the denotational content of Hannan and Miller's CLS machine and of Landin's SECD machine. We formally compare the corresponding evaluators and we illustrate some degrees of freedom in the design spaces of evaluators and of abstract machines for the calculus with computational effects. Finally we consider the Categorical Abstract Machine and the extent to which it is more of a virtual machine than an abstract machine,"['abstract machines', 'transformation into continuation-passing style (CPS)', 'evaluator', 'Interpreters', 'call-by-value', 'call-by-name', 'abstract machine', 'closure conversion', 'defunctionalization']",162,9
11,12,Recent computer technologies have enabled fast high-quality 3D graphics on personal computers and also have made the development of 3D graphical applications easier. However  most of such technologies do not sufficiently support layout and behavior aspects of 3D graphics. Geometric constraints are in general a powerful tool for specifying layouts and behaviors of graphical objects and have been applied to 2D graphical user interfaces and specialized 3D graphics packages. In this paper we present Chorus3D a geometric constraint library for 3D graphical applications. It enables programmers to use geometric constraints for various purposes such as geometric layout constrained dragging and inverse kinematics. Its novel feature is to handle scene graphs by processing coordinate transformations in geometric constraint satisfaction. We demonstrate the usefulness of Chorus3D by presenting sample constraint-based 3D graphical applications.,"['geometric constraints', 'behaviors', 'graphical objects', 'scene graphs', 'constraint satisfaction', 'layout', 'coordinate transformation', 'geometric layout', '3D graphical applications', '3D graphics']",131,10
12,13,"Table is a commonly used presentation scheme especially for describing relational information. However table understanding remains an open problem. In this paper we consider the problem of table detection in web documents. Its potential applications include web mining knowledge management  and web content summarization and delivery to narrow-bandwidth devices. We describe a machine learning based approach to classify each given table entity as either genuine or non-genuine . Various features re ecting the layout as well as content characteristics of tables are studied. In order to facilitate the training and evaluation of our table classi er we designed a novel web document table ground truthing protocol and used it to build a large table ground truth database. The database consists of 1,393 HTML les collected from hundreds of di erent web sites and contains 11,477 leaf lt;TABLE&gt elements out of which 1,740 are genuine tables. Experiments were conducted using the cross validation method and an F-measure of 95  89 was achieved.","['Table detection', 'presentation', 'Layout Analysis', 'Support Vector Machine', 'table ground truthing protocol', 'Machine Learning', 'Decision tree', 'Table Detection', 'Layout', 'HTML document', 'machine learning based approach', 'word group', 'Information Retrieval', 'Algorithms', 'classifers', 'content type', 'classifcation schemes']",161,17
13,14,This paper is concerned with `intranet search'. By intranet search we mean searching for information on an intranet within an organization. We have found that search needs on an intranet can be categorized into types through an analysis of survey results and an analysis of search log data. The types include searching for definitions persons experts and homepages. Traditional information retrieval only focuses on search of relevant documents but not on search of special types of information. We propose a new approach to intranet search in which we search for information in each of the special types in addition to the traditional relevance search. Information extraction technologies can play key roles in such kind of `search by type approach because we must first extract from the documents the necessary information in each type. We have developed an intranet search system called `Information Desk'. In the system we try to address the most important types of search first  finding term definitions homepages of groups or topics employees personal information and experts on topics. For each type of search we use information extraction technologies to extract fuse and summarize information in advance. The system is in operation on the intranet of Microsoft and receives accesses from about 500 employees per month. Feedbacks from users and system logs show that users consider the approach useful and the system can really help people to find information. This paper describes the architecture features component technologies and evaluation results of the system.,"['definition search', 'Intranet search', 'information retrieval', 'Search Needs', 'architecture', 'metadata extraction', 'component technologies', 'Experimentation', 'information extraction', 'types of information', 'intranet search', 'expert finding', 'Human Factors', 'features', 'Algorithms', 'INFORMATION DESK']",246,16
14,15,A new statistical formula for identifying 2-character words in Chinese text called the contextual information formula was developed empirically by performing stepwise logistic regression using a sample of sentences that had been manually segmented. Contextual information in the form of the frequency of characters that are adjacent to the bigram being processed as well as the weighted document frequency of the overlapping bigrams were found to be significant factors for predicting the probablity that the bigram constitutes a word. Local information the number of times the bigram occurs in the document being segmented and the position of the bigram in the sentence were not found to be useful in determining words. The contextual information formula was found to be significantly and substantially better than the mutual information formula in identifying 2-character words. The method can also be used for identifying multi-word terms in English text.,"['multi-word terms', 'mutual information', 'word boundary identification', 'regression model', 'Chinese text segmentation', 'contextual information', 'natural language processing', 'statistical formula', 'logistic regression', 'word boundary']",145,10
15,16,Programming languages are a part of the core of computer science. Courses on programming languages are typically offered to junior or senior students and textbooks are based on this assumption. However our computer science curriculum offers the programming languages course in the first year. This unusual situation led us to design it from an untypical approach. In this paper we first analyze and classify proposals for the programming languages course into different pure and hybrid approaches. Then we describe a course for freshmen based on four pure approaches and justify the main choices made. Finally we identify the software used for laboratories and outline our experience after teaching it for seven years.,"['programming paradigms', 'laboratory component', 'formal grammars', 'programming language course', 'recursion', 'freshmen', 'computer science', 'topics', 'Programming languages', 'language description', 'programming methodology', 'curriculum', 'functional programming']",112,13
16,17,The emergence of Bluetooth as a default radio interface allows handheld devices to be rapidly interconnected into ad hoc networks. Bluetooth allows large numbers of piconets to form a scatternet using designated nodes that participate in multiple piconets. A unit that participates in multiple piconets can serve as a bridge and forwards traffic between neighbouring piconets. Since a Bluetooth unit can transmit or receive in only one piconet at a time a bridging unit has to share its time among the different piconets. To schedule communication with bridging nodes one must take into account their availability in the different piconets which represents a difficult  scatternet wide coordination problem and can be an important performance bottleneck in building scatternets. In this paper we propose the Pseudo-Random Coordinated Scatternet Scheduling PCSS algorithm to perform the scheduling of both intra and inter-piconet communication. In this algorithm Bluetooth nodes assign meeting points with their peers such that the sequence of meeting points follows a pseudo random process that is different for each pair of nodes. The uniqueness of the pseudo random sequence guarantees that the meeting points with different peers of the node will collide only occasionally. This removes the need for explicit information exchange between peer devices which is a major advantage of the algorithm. The lack of explicit signaling between Bluetooth nodes makes it easy to deploy the PCSS algorithm in Bluetooth devices while conformance to the current Bluetooth specification is also maintained. To assess the performance of the algorithm we define two reference case schedulers and perform simulations in a number of scenarios where we compare the performance of PCSS to the performance of the reference schedulers.,"['bridging unit', 'checkpoint', 'slaves', 'threshold', 'intensity', 'piconets', 'inter-piconet communication', 'scatternet', 'total utilization', 'PCSS algorithm', 'Network Access Point', 'scheduling', 'Bluetooth']",276,13
17,18,This paper focuses on defending against compromised nodes dropping of legitimate reports and investigates the misbehavior of a maliciously packet-dropping node in sensor networks . We present a resilient packet-forwarding scheme using Neighbor Watch System NWS specifically designed for hop-by-hop reliable delivery in face of malicious nodes that drop relaying packets as well as faulty nodes that fail to relay packets. Unlike previous work with multipath data forwarding our scheme basically employs single-path data forwarding which consumes less power than multipath schemes. As the packet is forwarded along the single-path toward the base station our scheme however converts into multipath data forwarding at the location where NWS detects relaying nodes misbehavior. Simulation experiments show that with the help of NWS our forwarding scheme achieves a high success ratio in face of a large number of packet-dropping nodes and effectively adjusts its forwarding style depending on the number of packet-dropping nodes en-route to the base station.,"['Reliable Delivery', 'cluster key', 'single-path forwarding', 'Packet-dropping Attacks', 'degree of multipath', 'Secure Routing', 'Neighbor Watch System', 'robustness', 'aggregation protocols', 'Sensor Network Security', 'malicious node', 'critical area', 'secure ad-hoc network routing protocol', 'legitimate node']",155,14
18,19,Recognition of motion streams such as data streams generated by different sign languages or various captured human body motions requires a high performance similarity measure . The motion streams have multiple attributes and motion patterns in the streams can have different lengths from those of isolated motion patterns and different attributes can have different temporal shifts and variations. To address these issues this paper proposes a similarity measure based on singular value decomposition SVD of motion matrices . Eigenvector differences weighed by the corresponding eigenvalues are considered for the proposed similarity measure . Experiments with general hand gestures and human motion streams show that the proposed similarity measure gives good performance for recognizing motion patterns in the motion streams in real time.,"['segmentation', 'gesture', 'similarity measure', 'eigenvector', 'recognition', 'eigenvalue', 'motion stream', 'data streams', 'singular value decomposition', 'Pattern recognition']",122,10
19,20,Information seeking and management practices are an integral aspect of people's daily work. However we still have little understanding of collaboration in the information seeking process. Through a survey of collaborative information seeking practices of academic researchers we found that researchers reported that 1 the lack of expertise is the primary reason that they collaborate when seeking information 2 traditional methods including face-to-face phone and email are the preferred communication mediums for collaboration and 3 collaborative information seeking activities are usually successful and more useful than individually seeking information. These results begin to highlight the important role that collaborative information seeking plays in daily work.,"['Group Work', 'collaboration', 'Academic Researchers', 'Collaborative Information Seeking', 'Survey', 'communication media', 'information seeking']",105,7
20,21,Researchers have explored the design of ambient information systems across a wide range of physical and screen-based media. This work has yielded rich examples of design approaches to the problem of presenting information about a user's world in a way that is not distracting but is aesthetically pleasing and tangible to varying degrees. Despite these successes accumulating theoretical and craft knowledge has been stymied by the lack of a unified vocabulary to describe these systems and a consequent lack of a framework for understanding their design attributes. We argue that this area would significantly benefit from consensus about the design space of ambient information systems and the design attributes that define and distinguish existing approaches. We present a definition of ambient information systems and a taxonomy across four design dimensions Information Capacity Notification Level Representational Fidelity and Aesthetic Emphasis. Our analysis has uncovered four patterns of system design and points to unexplored regions of the design space which may motivate future work in the field.,"['Notification System', 'notification systems and peripheral displays', 'Ubiquitous Computing', 'multiple-information consolidators', 'ambient information system', 'Design Guidelines', 'Peripheral Display', 'definition and characteristics of ambient systems', 'Taxonomy', 'information monitor display', 'Ambient Display', 'high throughput textual display', 'four main design patterns', 'framework to understand design attributes', 'symbolic Sculptural display', 'user interface', 'calm computing']",165,17
21,22,Hidden Web databases maintain a collection of specialised documents which are dynamically generated in response to users queries. However the documents are generated by Web page templates which contain information that is irrelevant to queries. This paper presents a Two-Phase Sampling 2PS technique that detects templates and extracts query-related information from the sampled documents of a database. In the first phase 2PS queries databases with terms contained in their search interface pages and the subsequently sampled documents. This process retrieves a required number of documents. In the second phase 2PS detects Web page templates in the sampled documents in order to extract information relevant to queries. We test 2PS on a number of real-world Hidden Web databases. Experimental results demonstrate that 2PS effectively eliminates irrelevant information contained in Web page templates and generates terms and frequencies with improved accuracy.,"['Document Sampling', 'irrelavant information extraction', 'search interface pages', 'Information Extraction', 'query-based sampling', 'neighbouring adjacent tag segments', 'Hidden Web Databases', 'web page templates', 'information extraction', '2-phase sampling technique', 'string matching techniques', 'hidden web databases', 'hypertext markup langauges']",139,13
22,23,In this paper we introduce a unified approach for the adaptive control of 3G mobile networks in order to improve both quality of service QoS for mobile subscribers and to increase revenue for service providers. The introduced approach constantly monitors QoS measures as packet loss probability and the current number of active mobile users during operation of the network. Based on the values of the QoS measures just observed the system parameters of the admission controller and packet scheduler are controlled by the adaptive performance management entity. Considering UMTS we present performance curves showing that handover failure probability is improved by more than one order of magnitude. Moreover the packet loss probability can be effectively regulated to a predefined level and provider revenue is significantly increased for all pricing policies.,"['pricing and revenue optimization', 'QoS', 'admission control', '3G mobile networks', 'Quality of Service in mobile systems', 'packet loss probability', 'adaptive performance management', 'provider revenue', 'packet scheduler', 'performance evaluation of next generation mobile systems', 'pricing policy', 'admission control in mobile system']",130,12
23,24,Facet-based component retrieval techniques have been proved to be an effective way for retrieving. These Techniques are widely adopted by component library systems but they usually simply list out all the retrieval results without any kind of ranking. In our work we focus on the problem that how to determine the ranks of the components retrieved by user. Factors which can influence the ranking are extracted and identified through the analysis of ER-Diagram of facet-based component library system. In this paper a mathematical model of weighted ranking algorithm is proposed and the timing of ranks calculation is discussed. Experiment results show that this algorithm greatly improves the efficiency of component retrieval system.,"['Weighted ranking algorithm', 'and component library', 'retrieval system', 'facet', 'component retrieval', 'component rank', 'ranking algorithm', 'matching degree', 'component library', 'facet-based component retrieval']",112,10
24,25,The organization of HTML into a tag tree structure which is rendered by browsers as roughly rectangular regions with embedded text and HREF links greatly helps surfers locate and click on links that best satisfy their information need. Can an automatic program emulate this human behavior and thereby learn to predict the relevance of an unseen HREF target page w.r.t. an information need based on information limited to the HREF source page Such a capability would be of great interest in focused crawling and resource discovery because it can fine-tune the priority of unvisited URLs in the crawl frontier and reduce the number of irrelevant pages which are fetched and discarded. We show that there is indeed a great deal of usable information on a HREF source page about the relevance of the target page. This information encoded suitably can be exploited by a supervised apprentice which takes online lessons from a traditional focused crawler by observing a carefully designed set of features and events associated with the crawler. Once the apprentice gets a sufficient number of examples the crawler starts consulting it to better prioritize URLs in the crawl frontier. Experiments on a dozen topics using a 482-topic taxonomy from the Open Directory Dmoz show that online relevance feedback can reduce false positives by 30 to 90%.,"['classifiers', 'HREF link', 'focused crawlers', 'Document object model', 'URLs', 'Focused crawling', 'taxonomy', 'Reinforcement learning', 'DOM']",218,9
25,26,Many volume filtering operations used for image enhancement data processing or feature detection can be written in terms of three-dimensional convolutions. It is not possible to yield interactive frame rates on todays hardware when applying such convolutions on volume data using software filter routines. As modern graphics workstations have the ability to render two-dimensional convoluted images to the frame buffer this feature can be used to accelerate the process significantly. This way generic 3D convolution can be added as a powerful tool in interactive volume visualization toolkits.,"['3D convolution', 'Hardware Acceleration', 'Volume Visualization', 'volume rendering', 'visualization', 'filtering', 'Convolution']",87,7
26,27,"As today the amount of accessible information is overwhelming the intelligent and personalized filtering of available information is a main challenge. Additionally there is a growing need for the seamless mobile and multi-modal system usage throughout the whole day to meet the requirements of the modern society anytime anywhere anyhow""). A personal information agent that is delivering the right information at the right time by accessing filtering and presenting information in a situation-aware matter is needed. Applying Agent-technology is promising because the inherent capabilities of agents like autonomy pro and reactiveness offer an adequate approach. We developed an agent-based personal information system called PIA for collecting filtering and integrating information at a common point offering access to the information by WWW e-mail SMS MMS and J2ME clients. Push and pull techniques are combined allowing the user to search explicitly for specific information on the one hand and to be informed automatically about relevant information divided in pre work and recreation slots on the other hand. In the core of the PIA system advanced filtering techniques are deployed through multiple filtering agent communities for content-based and collaborative filtering. Information-extracting agents are constantly gathering new relevant information from a variety of selected sources internet files databases web-services etc.). A personal agent for each user is managing the individual information provisioning tailored to the needs of this specific user knowing the profile the current situation and learning from feedback.","['Ubiquitous access', 'Adaptation and Learning', 'personal information agent', 'agent technology', 'Recommendation systems', 'Intelligent and personalized filtering', 'filtering', 'Agent-based deployed applications', 'Evolution', 'Agents and complex systems']",236,10
27,28,In this paper we present a multilingual information retrieval system that provides access to Tourism information by exploiting the intuitiveness of natural language. In particular we describe the knowledge representation model underlying the information retrieval system. This knowledge representation approach is based on associative networks and allows the definition of semantic relationships between domain-intrinsic information items. The network structure is used to define weighted associations between information items and augments the system with a fuzzy search strategy. This particular search strategy is performed by a constrained spreading activation algorithm that implements information retrieval on associative networks. Strictly speaking we take the relatedness of terms into account and show how this fuzzy search strategy yields beneficial results and moreover determines highly associated matches to users queries. Thus the combination of the associative network and the constrained spreading activation approach constitutes a search algorithm that evaluates the relatedness of terms and therefore provides a means for implicit query expansion.,"['multilingual information retrieval system', 'spreading activation', 'knowledge representation model', 'constrained spreading activation', 'knowledge representation', 'natural language information retrieval', 'query expansion', 'natural language query', 'associative networks']",157,9
28,29,The dramatic increase in demand for wireless Internet access has lead to the introduction of new wireless architectures and systems including 3G Wi-Fi and WiMAX. 3G systems such as UMTS and CDMA2000 are leaning towards an all-IP architecture for transporting IP multimedia services mainly due to its scalability and promising capability of inter-working heterogeneous wireless access networks. During the last ten years substantial work has been done to understand the nature of wired IP traffic and it has been proven that IP traffic exhibits self-similar properties and burstiness over a large range of time scales. Recently because of the large deployment of new wireless architectures researchers have focused their attention towards understanding the nature of traffic carried by different wireless architecture and early studies have shown that wireless data traffic also exhibits strong long-range dependency. Thus the classical tele-traffic theory based on a simple Markovian process cannot be used to evaluate the performance of wireless networks. Unfortunately the area of understanding and modeling of different kinds of wireless traffic is still immature which constitutes a problem since it is crucial to guarantee tight bound QoS parameters to heterogeneous end users of the mobile Internet. In this paper we make several contributions to the accurate modeling of wireless IP traffic by presenting a novel analytical model that takes into account four different classes of self-similar traffic. The model consists of four queues and is based on a G/M/1 queueing system. We analyze it on the basis of priority with no preemption and find exact packet delays. To date no closed form expressions have been presented for G/M/1 with priority.,"['Self-Similar', '3G', 'wireless IP traffic', 'self-similar traffic', 'QoS', '3G networks', 'traffic modelling', 'queuing model', 'GGSN', 'UMTS']",268,10
29,30,Researchers with deep knowledge of scientific domains are now developing highly-adaptive and irregular asymmetrical  parallel computations leading to challenges in both delivery of data for computation and mapping of processes to physical resources. Using software engineering principles we have developed a new communications protocol and architectural style for asymmetrical parallel computations called ADaPT. Utilizing the support of architecturally-aware middleware we show that ADaPT provides a more efficient solution in terms of message passing and load balancing than asymmetrical parallel computations using collective calls in the Message-Passing Interface MPI or more advanced frameworks implementing explicit load-balancing policies. Additionally  developers using ADaPT gain significant windfall from good practices in software engineering including implementation-level support of architectural artifacts and separation of computational loci from communication protocols,"['collective calls', 'load balancing', 'ADaPT', 'Asymmetrical Parallel Computations', 'High-Performance Computing', 'high-performance computing', 'software engineering', 'communication protocols', 'asymamtrical parallel computations', 'MPI']",123,10
30,31,"Research in bioinformatics is driven by the experimental data. Current biological databases are populated by vast amounts of experimental data. Machine learning has been widely applied to bioinformatics and has gained a lot of success in this research area. At present with various learning algorithms available in the literature researchers are facing difficulties in choosing the best method that can apply to their data. We performed an empirical study on 7 individual learning systems and 9 different combined methods on 4 different biological data sets and provide some suggested issues to be considered when answering the following questions i How does one choose which algorithm is best suitable for their data set ii Are combined methods better than a single approach iii How does one compare the effectiveness of a particular algorithm to A href=""31.html#1"">the others","['biological data', 'bioinformatics', 'training data', 'classification', 'ensemble methods', 'performance evaluation', 'machine learning', 'cross validation', 'supervised machine learning', 'Supervised machine learning']",136,10
31,32,C applications in particular those using operating system level services frequently comprise multiple crosscutting concerns  network protocols and security are typical examples of such concerns. While these concerns can partially be addressed during design and implementation of an application they frequently become an issue at runtime e.g. to avoid server downtime. A deployed network protocol might not be efficient enough and may thus need to be replaced. Buffer overflows might be discovered that imply critical breaches in the security model of an application. A prefetching strategy may be required to enhance performance. While aspect-oriented programming seems attractive in this context none of the current aspect systems is expressive and efficient enough to address such concerns. This paper presents a new aspect system to provide a solution to this problem. While efficiency considerations have played an important part in the design of the aspect language the language allows aspects to be expressed more concisely than previous approaches. In particular it allows aspect programmers to quantify over sequences of execution points as well as over accesses through variable aliases. We show how the former can be used to modularize the replacement of network protocols and the latter to prevent buffer overflows. We also present an implementation of the language as an extension of Arachne a dynamic weaver for C applications. Finally we present performance evaluations supporting that Arachne is fast enough to extend high performance applications  such as the Squid web cache. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and/or a fee.,"['web cache', 'operating system', 'C applications', 'buffer overflows', 'sequence pointcut', 'aspect language', 'prefetching', 'Arachne', 'system applications', 'network protocol', 'binary code', 'dynamic weaving']",311,12
32,33,It is important for successful electronic business to have a hi-quality business website. So we need an accurate and effective index system to evaluate and analyses the quality of the business website. In this paper the evaluation index system following the `grey box principle is proposed which considers both efficiency of business website and performance of electronic business system. Using R-Hierarchical clustering method to extract the typical indexes from sub-indexes is theoretically proved to have a rationality and effectiveness. Finally the evaluation method is briefly discussed.,"['index optimization', 'e-commerce', 'Evaluation system', 'System performance', 'quality evaluation index', 'R-Hierarchical clustering method', 'R-Hierarchical clustering', 'clustering', 'correlation index', 'representitive indexes', 'fuzzy analysis', 'quality synthesis evaluation', 'B2C websites', 'Business website']",86,14
33,34,In this paper we present an expressive 3D animation environment that enables users to rapidly and visually prototype animated worlds with a fully 3D user-interface. A 3D device allows the specification of complex 3D motion while virtual tools are visible mediators that live in the same 3D space as application objects and supply the interaction metaphors to control them. In our environment there is no intrinsic difference between user interface and application objects. Multi-way constraints provide the necessary tight coupling among components that makes it possible to seamlessly compose animated and interactive behaviors. By recording the effects of manipulations all the expressive power of the 3D user interface is exploited to define animations. Effective editing of recorded manipulations is made possible by compacting all continuous parameter evolutions with an incremental data-reduction algorithm designed to preserve both geometry and timing. The automatic generation of editable representations of interactive performances overcomes one of the major limitations of current performance animation systems. Novel interactive solutions to animation problems are made possible by the tight integration of all system components. In particular animations can be synchronized by using constrained manipulation during playback. The accompanying video-tape illustrates our approach with interactive sequences showing the visual construction of 3D animated worlds. All the demonstrations in the video were recorded live and were not edited.,"['3D Interaction', 'dynamic model', '3D Widgets', '3d animation environment', '3D Animation', 'computer graphics', 'visualization', 'animation synchronization', 'Data Reduction', 'Virtual Tools', 'human interaction', 'data reduction', 'recording 3d manipulation', 'multi-way constrained architecture', '3d user interface', 'Local Propagation Constraints', 'Object-Oriented Graphics']",218,17
34,35,When testing database applications in addition to creating in-memory fixtures it is also necessary to create an initial database state that is appropriate for each test case. Current approaches either require exact database states to be specified in advance or else generate a single initial state under guidance from the user that is intended to be suitable for execution of all test cases. The first method allows large test suites to be executed in batch but requires considerable programmer effort to create the test cases and to maintain them). The second method requires less programmer effort but increases the likelihood that test cases will fail in non-fault situations due to unexpected changes to the content of the database. In this paper we propose a new approach in which the database states required for testing are specified intensionally as constrained queries that can be used to prepare the database for testing automatically . This technique overcomes the limitations of the other approaches and does not appear to impose significant performance overheads.,"['Testing Framework', 'Lesser Programmer Effort for Test Cases', 'software testing', 'Seamless Integration', 'Query Based Language', 'Efficient Testing', 'Performance Testing', 'databases', 'Improvement for the Intensional Test Cases', 'Intensional Test Cases', 'database testing', 'Testing for Database Systems', 'DOT-UNIT']",170,13
35,36,A neural network based clustering method for the analysis of soft handovers in 3G network is introduced. The method is highly visual and it could be utilized in explorative analysis of mobile networks. In this paper the method is used to find groups of similar mobile cell pairs in the sense of handover measurements. The groups or clusters found by the method are characterized by the rate of successful handovers as well as the causes of failing handover attempts. The most interesting clusters are those which represent certain type of problems in handover attempts. By comparing variable histograms of a selected cluster to histograms of the whole data set an application domain expert may find some explanations on problems. Two clusters are investigated further and causes of failing handover attempts are discussed.,"['soft handover', 'neural networks', 'Cluster Analysis', 'Self-Organizing Map', 'data mining', 'Visualization Capability', 'Data Mining', 'Two-Phase Clustering Algorithm', 'Soft Handover', 'hierarchical clustering', 'Decrease in Computational Complexity', 'Key Performance Indicator of Handover', 'mobility management', 'Histograms', 'Neural Network Algorithm', '3G network']",132,16
36,37,Aspect Oriented Programming a relatively new programming paradigm earned the scientific community's attention . The paradigm is already evaluated for traditional OOP and component-based software development with remarkable results. However most of the published work while of excellent quality is mostly theoretical or involves evaluation of AOP for research oriented and experimental software . Unlike the previous work this study considers the AOP paradigm for solving real-life problems which can be faced in any commercial software. We evaluate AOP in the development of a high-performance component-based web-crawling system and compare the process with the development of the same system without AOP. The results of the case study mostly favor the aspect oriented paradigm.,"['programming paradigms', 'Aspect Oriented Programming', 'evaluation', 'Software development process experiment', 'component-based application', 'Object Oriented Programming', 'AOP', 'Aspect Oriented Programming application', 'Web Crawler Implementation', 'OOP', 'case study', 'programming paradigm comparison', 'development process experiment metrics']",113,13
37,38,In a large sensor network in-network data aggregation i.e. combining partial results at intermediate nodes during message routing significantly reduces the amount of communication and hence the energy consumed. Recently several researchers have proposed robust aggregation frameworks which combine multi-path routing schemes with duplicate-insensitive algorithms to accurately compute aggregates e.g. Sum Count Average in spite of message losses resulting from node and transmission failures. However these aggregation frameworks have been designed without security in mind. Given the lack of hardware support for tamper-resistance and the unattended nature of sensor nodes sensor networks are highly vulnerable to node compromises. We show that even if a few compromised nodes contribute false sub-aggregate values this results in large errors in the aggregate computed at the root of the hierarchy. We present modifications to the aggregation algorithms that guard against such attacks i.e. we present algorithms for resilient hierarchical data aggregation despite the presence of compromised nodes in the aggregation hierarchy. We evaluate the performance and costs of our approach via both analysis and simulation . Our results show that our approach is scalable and efficient.,"['Attack-Resilient', 'Hierarchical Aggregation', 'network aggregation algorithms', 'Count aggregate', 'robust aggregation', 'sensor networks', 'in-network data aggregation', 'node compromise prevention', 'Sum aggregate', 'Data Aggregation', 'falsified local value attack', 'Attack resilient heirarchical data aggregation', 'falsified sub-aggregate attack', 'synopsis diffusion aggregation framework', 'Synopsis Diffusion', 'Sensor Network Security']",182,16
38,39,To have a rich presentation of a topic it is not only expected that many relevant multimodal information including images text audio and video could be extracted it is also important to organize and summarize the related information and provide users a concise and informative storyboard about the target topic. It facilitates users to quickly grasp and better understand the content of a topic. In this paper we present a novel approach to automatically generating a rich presentation of a given semantic topic. In our proposed approach the related multimodal information of a given topic is first extracted from available multimedia databases or websites. Since each topic usually contains multiple events a text-based event clustering algorithm is then performed with a generative model. Other media information such as the representative images possibly available video clips and flashes interactive animates are associated with each related event. A storyboard of the target topic is thus generated by integrating each event and its corresponding multimodal information. Finally to make the storyboard more expressive and attractive an incidental music is chosen as background and is aligned with the storyboard. A user study indicates that the presented system works quite well on our testing examples.,"['multimedia authoring', 'High-level semantics', 'Rich video clips and flashes', 'Generate storyboard', 'Multi-modal information', 'Event clustering', 'documentary and movie', 'multimodality', 'Rich presentation', 'Images, videos and Audio Technologies', 'Communication and Multimedia', 'Representative Media', 'Subjective multiple events', 'storyboard', 'multimedia fusion', 'events clustering']",200,16
39,40,In this paper we propose a machine learning approach to title extraction from general documents. By general documents we mean documents that can belong to any one of a number of specific genres including presentations book chapters technical papers brochures reports and letters. Previously methods have been proposed mainly for title extraction from research papers. It has not been clear whether it could be possible to conduct automatic title extraction from general documents. As a case study we consider extraction from Office including Word and PowerPoint. In our approach we annotate titles in sample documents for Word and PowerPoint respectively and take them as training data train machine learning models and perform title extraction using the trained models. Our method is unique in that we mainly utilize formatting information such as font size as features in the models. It turns out that the use of formatting information can lead to quite accurate extraction from general documents. Precision and recall for title extraction from Word is 0.810 and 0.837 respectively and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data. Other important new findings in this work include that we can train models in one domain and apply them to another domain and more surprisingly we can even train models in one language and apply them to another language. Moreover we can significantly improve search ranking results in document retrieval by using the extracted titles.,"['Search ranking results', 'File Formats’ extraction', 'File extraction', 'metadata extraction', 'Metadata processing', 'search', 'generic languages', 'information extraction', 'machine learning', 'Digital Copies', 'PowerPoint documents', 'Information search and Retrieval', 'Precision extraction', 'Microsoft Office Automation']",244,14
40,41,Intrusion or misbehaviour detection systems are an important and widely accepted security tool in computer and wireless sensor networks. Their aim is to detect misbehaving or faulty nodes in order to take appropriate countermeasures thus limiting the damage caused by adversaries as well as by hard or software faults. So far however once detected misbehaving nodes have just been isolated from the rest of the sensor network and hence are no longer usable by running applications. In the presence of an adversary or software faults this proceeding will inevitably lead to an early and complete loss of the whole network. For this reason we propose to no longer expel misbehaving nodes but to recover them into normal operation. In this paper we address this problem and present a formal specification of what is considered a secure and correct node recovery algorithm together with a distributed algorithm that meets these properties. We discuss its requirements on the soft and hardware of a node and show how they can be fulfilled with current and upcoming technologies. The algorithm is evaluated analytically as well as by means of extensive simulations and the findings are compared to the outcome of a real implementation for the BTnode sensor platform. The results show that recovering sensor nodes is an expensive though feasible and worthwhile task. Moreover  the proposed program code update algorithm is not only secure but also fair and robust.,"['Intrusion Detection', 'distributed algorithm', 'Node Recovery', 'IDS', 'sensor networks', 'security', 'countermeasures', 'Wireless Sensor Networks', 'intrusion detection', 'sensor nodes', 'node recovery']",235,11
41,42,This paper explores the use of Bayesian online classifiers to classify text documents. Empirical results indicate that these classifiers are comparable with the best text classification systems. Furthermore the online approach offers the advantage of continuous learning in the batch-adaptive text filtering task.,"['Text Filtering', 'continous learning', 'Machine Learning', 'text classification', 'Bayesian', 'Online', 'Text Classification', 'machine learning', 'filtering', 'information gain', 'perceptron', 'Bayesian online classifiers', 'Gaussian process']",43,13
42,43,Since the publication of Brin and Page's paper on PageRank many in the Web community have depended on PageRank for the static query-independent ordering of Web pages. We show that we can significantly outperform PageRank using features that are independent of the link structure of the Web. We gain a further boost in accuracy by using data on the frequency at which users visit Web pages. We use RankNet a ranking machine learning algorithm to combine these and other static features based on anchor text and domain characteristics. The resulting model achieves a static ranking pairwise accuracy of 67.3 vs. 56.7 for PageRank or 50 for random).,"['Static ranking', 'relevance', 'static features', 'RankNet', 'dynamic ranking', 'fRank', 'static ranking', 'pairwise accuracy', 'popularity data', 'anchor text', 'Web pages', 'search engines', 'PageRank']",107,13
43,44,It is well known that the secure computation of non-trivial functionalities in the setting of no honest majority requires computational assumptions. We study the way such computational assumptions are used. Specifically we ask whether the secure protocol can use the underlying primitive e.g. one-way trapdoor permutation in a black-box way or must it be nonblack-box by referring to the code that computes this primitive Despite the fact that many general constructions of cryptographic schemes e.g. CPA-secure encryption  refer to the underlying primitive in a black-box way only there are some constructions that are inherently nonblack-box. Indeed all known constructions of protocols for general secure computation that are secure in the presence of a malicious adversary and without an honest majority use the underlying primitive in a nonblack-box way requiring to prove in zero-knowledge statements that relate to the primitive). In this paper we study whether such nonblack-box use is essential. We present protocols that use only black-box access to a family of enhanced trapdoor permutations or to a homomorphic public-key encryption scheme. The result is a protocol whose communication complexity is independent of the computational complexity of the underlying primitive e.g. a trapdoor permutation and whose computational complexity grows only linearly with that of the underlying primitive. This is the first protocol to exhibit these properties.,"['nonblack-box', 'oblivious transfer protocol', 'malicious adversary', 'black-box', 'trapdoor permutation', 'secure computation', 'cryptographic', 'encryption scheme', 'oblivious transfer', 'black-box reductions', 'Theory of cryptography']",216,11
44,45,Bluetooth is a cable replacement technology for Wireless Personal Area Networks. It is designed to support a wide variety of applications such as voice streamed audio and video web browsing printing and file sharing each imposing a number of quality of service constraints including packet loss latency delay variation and throughput. In addition to QOS support another challenge for Bluetooth stems from having to share the 2.4 GHz ISM band with other wireless devices such as IEEE 802.11. The main goal of this paper is to investigate the use of a dynamic scheduling algorithm that guarantees QoS while reducing the impact of interference. We propose a mapping between some common QoS parameters such as latency and bit rate and the parameters used in the algorithm. We study the algorithm's performance and obtain simulation results for selected scenarios and configurations of interest.,"['coexistence', 'interference', 'QoS', 'MAC scheduling', 'scheduling priorities', 'BIAS', 'inteference', 'WPANs', 'WPAN', 'WLAN', 'Bluetooth', 'dynamic scheduling']",141,12
45,46,This paper examines the average page quality over time of pages downloaded during a web crawl of 328 million unique pages. We use the connectivity-based metric PageRank to measure the quality of a page. We show that traversing the web graph in breadth-first search order is a good crawling strategy as it tends to discover high-quality pages early on in the crawl.,"['connectivity-based metrics', '', 'high quality pages', 'breadth-first search', 'ordering metrics', 'page quality metric', 'crawling', 'crawl order', 'Crawling', 'breadth first search', 'PageRank']",62,11
46,47,"Many instant messenger IM clients let a person specify the identifying name that appears in another person's contact list. We have noticed that many people add extra information to this name as a way to broadcast information to their contacts. Twelve IM contact lists comprising 444 individuals were monitored over three weeks to observe how these individuals used and altered their display names. Almost half of them changed their display names at varying frequencies where the new information fell into seventeen different categories of communication supplied to others. Three themes encompass these categories Identification who am I Information About Self this is what is going on with me and Broadcast Message I am directing information to the community""). The design implication is that systems supporting person to person casual interaction such as IM should explicitly include facilities that allow people to broadcast these types of information to their community of contacts.","['Related IM Research', 'communication', 'Communication Catogories', 'display name', 'Name Variation Handles', 'Catorgorisation Of Display Names', 'Instant Messaging', 'Broadcast Information', 'Identification Is Fundamental', 'MSN messager', 'awareness', 'Distribution Frequency Of Various Catogories', 'Instant messenger', 'Display Names']",151,14
47,48,This paper describes the building of a research library for studying the Web especially research on how the structure and content of the Web change over time. The library is particularly aimed at supporting social scientists for whom the Web is both a fascinating social phenomenon and a mirror on society. The library is built on the collections of the Internet Archive which has been preserving a crawl of the Web every two months since 1996. The technical challenges in organizing this data for research fall into two categories high-performance computing to transfer and manage the very large amounts of data and human-computer interfaces that empower research by non-computer specialists.,"['Storage', 'User Support', 'history of the Web', 'Full Text Indexes', 'Flexible Preload System', 'computational social science', 'digital libraries', 'Scalability', 'Database Access', 'Internet Archive', 'Database Management', 'Dataflow', 'User Interface']",110,13
48,49,This working group laid the groundwork for the collection and analysis of oral histories of women computing educators. This endeavor will eventually create a body of narratives to serve as role models to attract students in particular women to computing it will also serve to preserve the history of the female pioneers in computing education. Pre-conference work included administration of a survey to assess topical interest. The working group produced aids for conducting interviews including an opening script an outline of topics to be covered guidelines for conducting interviews and a set of probing questions to ensure consistency in the interviews. The group explored issues such as copyright and archival that confront the large-scale implementation of the project and suggested extensions to this research. This report includes an annotated bibliography of resources. The next steps will include training colleagues in how to conduct interviews and establishing guidelines for archival and use of the interviews.,"['Computing Education History', 'Oral History']",154,2
49,50,This paper introduces a rationale for and approach to the study of sustainability in computerized community information systems. It begins by presenting a theoretical framework for posing questions about sustainability predicated upon assumptions from social construction of technology and adaptive structuration theories. Based in part on the literature and in part on our own experiences in developing a community information system we introduce and consider three issues related to sustainability stakeholder involvement commitment from key players and the development of critical mass.,"['critical mass', 'computerized community information system', 'key players', 'construction of technology', 'participatory design', 'Community networks', 'community network', 'sustainability', 'skateholder involvement']",82,9
50,51,Machine learning systems offer unparalled flexibility in dealing with evolving input in a variety of applications such as intrusion detection systems and spam e-mail filtering. However  machine learning algorithms themselves can be a target of attack by a malicious adversary. This paper provides a framework for answering the question Can machine learning be secure Novel contributions of this paper include a taxonomy of different types of attacks on machine learning techniques and systems a variety of defenses against those attacks a discussion of ideas that are important to security for machine learning an analytical model giving a lower bound on attacker's work function and a list of open problems.,"['Intrusion Detection', 'Intrusion detection system', 'Targeted attack', 'Availability', 'Machine learning', 'Computer Security', 'Machine Learning', 'Causative attack', 'Learning algorithms', 'Game Theory', 'Statistical Learning', 'Indiscriminate attack', 'Spam Filters', 'Security Metrics', 'Adversarial Learning', 'Exploratory attack', 'Integrity', 'Security', 'Computer Networks']",109,19
51,52,The Catenaccio system integrates information retrieval with sketch manipulations. The system is designed especially for pen-based computing and allows users to retrieve information by simple pen manipulations such as drawing a picture. When a user draws a circle and writes a keyword information nodes related to the keyword are collected automatically inside the circle. In addition the user can create a Venn diagram by repeatedly drawing circles and keywords to form more complex queries. Thus the user can retrieve information both interactively and visually without complex manipulations. Moreover the sketch interaction is so simple that it is possible to combine it with other types of data such as images and real-world information for information retrieval. In this paper we describe our Catenaccio system and how it can be effectively applied.,"['interactive system', 'Interactive information retrieval system', 'keyword searching', '2D system', 'Sketch manipulation', 'Venn diagram', 'Pen-based computing', 'Image data', 'sketch manipulations', 'Catnaccio system', 'Visual information retrieval', 'Information retrieval', 'Information node']",130,13
52,53,Compression reduces both the size of indexes and the time needed to evaluate queries. In this paper we revisit the compression of inverted lists of document postings that store the position and frequency of indexed terms considering two approaches to improving retrieval efficiency:better implementation and better choice of integer compression schemes. First we propose several simple optimisations to well-known integer compression schemes and show experimentally that these lead to significant reductions in time. Second we explore the impact of choice of compression scheme on retrieval efficiency. In experiments on large collections of data we show two surprising results:use of simple byte-aligned codes halves the query evaluation time compared to the most compact Golomb-Rice bitwise compression schemes and even when an index fits entirely in memory byte-aligned codes result in faster query evaluation than does an uncompressed index emphasising that the cost of transferring data from memory to the CPU cache is less for an appropriately compressed index than for an uncompressed index. Moreover byte-aligned schemes have only a modest space overhead:the most compact schemes result in indexes that are around 10 of the size of the collection while a byte-aligned scheme is around 13%. We conclude that fast byte-aligned codes should be used to store integers in inverted lists.,"['Decoding', 'Document retrieval', 'index compression', 'integer coding', 'Inverted index', 'Efficiency', 'Optimisation', 'Bytewise compression', 'Compression', 'Variable byte', 'Inverted indexes', 'Search engine', 'Integer Compression', 'retrieval efficiency']",209,14
53,54,A consistent query answer in a possibly inconsistent database is an answer which is true in every minimal repair of the database. We present here a practical framework for computing consistent query answers for large possibly inconsistent relational databases. We consider relational algebra queries without projection  and denial constraints. Because our framework handles union queries we can effectively and efficiently extract indefinite disjunctive information from an inconsistent database. We describe a number of novel optimization techniques applicable in this context and summarize experimental results that validate our approach.,"['Inconsistent database', 'integrity constraints', 'Consistent Query answer', 'Denial constraints', 'Conflict hypergraph', 'query processing', 'Optimization', 'Polynomial time', 'Repair', 'Knowledge gathering', 'Disjunctive query', 'Relational algebra', 'inconsistency']",88,13
54,55,Research in consistent query answering studies the definition and computation of meaningful answers to queries posed to inconsistent databases i.e. databases whose data do not satisfy the integrity constraints ICs declared on their schema. Computing consistent answers to conjunctive queries is generally coNP-hard in data complexity even in the presence of very restricted forms of ICs single unary keys). Recent studies on consistent query answering for database schemas containing only key dependencies have an-alyzed the possibility of identifying classes of queries whose consistent answers can be obtained by a first-order rewriting of the query which in turn can be easily formulated in SQL and directly evaluated through any relational DBMS. In this paper we study consistent query answering in the presence of key dependencies and exclusion dependencies. We first prove that even in the presence of only exclusion dependencies the problem is coNP-hard in data complexity  and define a general method for consistent answering of conjunctive queries under key and exclusion dependencies based on the rewriting of the query in Datalog with negation . Then we identify a subclass of conjunctive queries that can be first-order rewritten in the presence of key and exclusion dependencies and define an algorithm for computing the first-order rewriting of a query belonging to such a class of queries. Finally we compare the relative efficiency of the two methods for processing queries in the subclass above mentioned. Experimental results conducted on a real and large database of the computer science engineering degrees of the University of Rome La Sapienza clearly show the computational advantage of the first-order based technique.,"['integrity constraints', 'query rewriting', 'inconsistent database', 'Computational Complexity', 'database schemas', 'Query Rewriting', 'conjunctive queries', 'consistent query answering', 'relational database', 'Inconsistency']",264,10
55,56,Apart from completeness usability performance and maintainability are the key quality aspects for Web information systems. Considering usability as key implies taking usage processes into account right from the beginning of systems development. Context-awareness appears as a promising idea for increasing usability of Web Information Systems. In the present paper we propose an approach to context-awareness of Web Information Systems that systematically distinguishes among the various important kinds of context. We show how parts of this context can be operationalized for increasing customers usage comfort. Our approach permits designing Web information systems such that they meet high quality expectations concerning usability performance and maintainability. We demonstrate the validity of our approach by discussing the part of a banking Web Information System dedicated to online home-loan application.,"['scenarios', 'web site', 'media type', 'web information system', 'context-aware information systems', 'SiteLang', 'context-awareness', 'story boarding', 'Web services', 'usability', 'scenes', 'Web Information Systems', 'media objects']",126,13
56,57,This paper discusses the problem of partial object recognition in image databases. We propose the method to reconstruct and estimate partially occluded shapes and regions of objects in images from overlapping and cutting. We present the robust method for recognizing partially occluded objects based on symmetry properties which is based on the contours of objects. Our method provides simple techniques to reconstruct occluded regions via a region copy using the symmetry axis within an object. Based on the estimated parameters for partially occluded objects we perform object recognition on the classification tree. Since our method relies on reconstruction of the object based on the symmetry rather than statistical estimates it has proven to be remarkably robust in recognizing partially occluded objects in the presence of scale changes rotation and viewpoint changes.,"['Image', 'Object', 'reconstruction', 'estimation', 'object recognition', 'contour', 'Symmetry', 'occlusion', 'symmetry', 'Recognition', 'Contour']",131,11
57,58,In this paper we explore the connection between clustering categorical data and entropy clusters of similar poi lower entropy than those of dissimilar ones. We use this connection to design an incremental heuristic algorithm COOLCAT  which is capable of efficiently clustering large data sets of records with categorical attributes and data streams. In contrast with other categorical clustering algorithms published in the past COOLCAT's clustering results are very stable for different sample sizes and parameter settings. Also the criteria for clustering is a very intuitive one since it is deeply rooted on the well-known notion of entropy. Most importantly COOLCAT is well equipped to deal with clustering of data streams continuously arriving streams of data point since it is an incremental algorithm capable of clustering new points without having to look at every point that has been clustered so far. We demonstrate the efficiency and scalability of COOLCAT by a series of experiments on real and synthetic data sets.,"['entropy', 'COOLCAT', 'incremental algorithm', 'clustering', 'data streams', 'categorical clustering', 'data stream']",159,7
58,59,This paper provides an account of new measures of coupling and cohesion developed to assess the reusability of Java components retrieved from the internet by a search engine. These measures differ from the majority of established metrics in two respects they reflect the degree to which entities are coupled or resemble each other and they take account of indirect couplings or similarities. An empirical comparison of the new measures with eight established metrics shows the new measures are consistently superior at ranking components according to their reusability.,"['Reusability', 'Java components', 'Cohesion Metric', 'Coupling', 'Cohesion', 'Spearman Rank Correlation', 'Intransitive Relation', 'Search Engine Technology', 'Linear Regression', 'Component search engine', 'Binary Quantity', 'Experimentary Comparsion']",87,12
59,60,It is important yet hard to identify navigational queries in Web search due to a lack of sufficient information in Web queries which are typically very short. In this paper we study several machine learning methods including naive Bayes model maximum entropy model support vector machine SVM and stochastic gradient boosting tree SGBT for navigational query identification in Web search. To boost the performance of these machine techniques we exploit several feature selection methods and propose coupling feature selection with classification approaches to achieve the best performance. Different from most prior work that uses a small number of features in this paper we study the problem of identifying navigational queries with thousands of available features extracted from major commercial search engine results Web search user click data query log and the whole Web's relational content. A multi-level feature extraction system is constructed. Our results on real search data show that 1 Among all the features we tested user click distribution features are the most important set of features for identifying navigational queries. 2 In order to achieve good performance machine learning approaches have to be coupled with good feature selection methods. We find that gradient boosting tree coupled with linear SVM feature selection is most effective. 3 With carefully coupled feature selection and classification approaches navigational queries can be accurately identified with 88.1 F1 score which is 33 error rate reduction compared to the best uncoupled system and 40 error rate reduction compared to a well tuned system without feature selection.,"['Support Vector Machine', 'Linear SVM Feature Ranking', 'Machine Learning', 'Information Gain', 'Naive Bayes Classifier', 'Navigational Query Classification', 'Multiple Level feature system', 'Stochastic Gradient Boosting Tree', 'Experiments Results', 'Maximum Entropy Classifier', 'Navigational and Informational query', 'Gradient Boosting Tree']",251,12
60,61,Functional verification is widely acknowledged as the bottleneck in the hardware design cycle. This paper addresses one of the main challenges of simulation based verification or dynamic verification  by providing a new approach for Coverage Directed Test Generation CDG). This approach is based on Bayesian networks and computer learning techniques. It provides an efficient way for closing a feedback loop from the coverage domain back to a generator that produces new stimuli to the tested design. In this paper we show how to apply Bayesian networks to the CDG problem. Applying Bayesian networks to the CDG framework has been tested in several experiments exhibiting encouraging results and indicating that the suggested approach can be used to achieve CDG goals.,"['Bayesian Networks', 'Most Probable Explanation', 'Coverage Analysis', 'Dynamic Bayesian Network', 'Markov Chain', 'bidirectional inferences', 'Functional Verification', 'Coverage directed test generation', 'design under test', 'conditional probability', 'Maximal A Posteriori', 'coverage model']",119,12
61,62,"An index connects readers with information. Creating an index for a single book is a time-honored craft. Creating an index for a massive library of HTML topics is a modern craft that has largely been discarded in favor of robust search engines. The authors show how they optimized a single-sourced index for collections of HTML topics printed books and PDF books. With examples from a recent index of 24,000 entries for 7,000 distinct HTML topics also published as 40 different PDF books the authors discuss the connections between modern technology and traditional information retrieval methods that made the index possible usable and efficient to create and maintain.","['indexing problems', 'Search', 'Online information', 'Indexing', 'SQL data type', 'primary index entry', 'automation', 'Human factors', 'HTML master index', 'Internally developed format', 'drop-down selection of terms', 'book indexes', 'Navigation', 'Information retrieval methods', 'Massive Master']",107,15
62,63,Many criteria can be used to evaluate the performance of supervised learning. Different criteria are appropriate in different settings and it is not always clear which criteria to use. A further complication is that learning methods that perform well on one criterion may not perform well on other criteria. For example SVMs and boosting are designed to optimize accuracy whereas neural nets typically optimize squared error or cross entropy. We conducted an empirical study using a variety of learning methods SVMs neural nets k-nearest neighbor bagged and boosted trees and boosted stumps to compare nine boolean classification performance metrics Accuracy Lift F-Score Area under the ROC Curve Average Precision Precision/Recall Break-Even Point Squared Error Cross Entropy and Probability Calibration. Multidimensional scaling MDS shows that these metrics span a low dimensional manifold. The three metrics that are appropriate when predictions are interpreted as probabilities squared error cross entropy and calibration lay in one part of metric space far away from metrics that depend on the relative order of the predicted values ROC area average precision break-even point and lift. In between them fall two metrics that depend on comparing predictions to a threshold accuracy and F-score. As expected maximum margin methods such as SVMs and boosted trees have excellent performance on metrics like accuracy  but perform poorly on probability metrics such as squared error. What was not expected was that the margin methods have excellent performance on ordering metrics such as ROC area and average precision. We introduce a new metric SAR that combines squared error accuracy and ROC area into one metric. MDS and correlation analysis shows that SAR is centrally located and correlates well with other metrics suggesting that it is a good general purpose metric to use when more specific criteria are not known.,"['Performance Evaluation', 'supervised learning', 'standard deviation', 'SVMs', 'squared error', 'algorithmns', 'Lift', 'Recall', 'backpropagation', 'Cross Entropy', 'performance metric', 'Euclidean distance', 'Precision', 'Supervised Learning', 'ROC', 'pairwise', 'Metrics', 'ordering metric']",296,18
63,64,Database Security course is an important part of the InfoSec curriculum. In many institutions this is not taught as an independent course. Parts of the contents presented in this paper are usually incorporated in other courses such as Network Security. The importance of database security concepts stems from the fact that a compromise of data at rest could expose an organization to a greater security threat than otherwise. Database vulnerabilities exposed recently in several high profile incidents would be a good reason to dedicate a full course to this important topic. In this paper we present key topics such as technologies for database protection access control multilevel security database vulnerabilities and defenses privacy and legal issues impact of policies and some well known secure database models.,"['database', 'Database', 'privacy', 'policy', 'buffer overflows', 'access control', 'encryption', 'multilevel security', 'CIA', 'authentication', 'inference channel', 'security', 'inference', 'polyinstantiation']",126,14
64,65,This paper addresses the problem of scatternet formation for single-hop Bluetooth based ad hoc networks with minimal communication overhead. We adopt the well-known structure de Bruijn graph to form the backbone of Bluetooth scatternet hereafter called dBBlue  such that every master node has at most seven slaves every slave node is in at most two piconets and no node assumes both master and slave roles. Our structure dBBlue also enjoys a nice routing property the diameter of the graph is Ç ´ÐÓ Òµ and we can find a path with at most Ç ´ÐÓ Òµ hops for every pair of nodes without any routing table . Moreover the congestion of every node is at most Ç ´ÐÓ Ò Òµ  assuming that a unit of total traffic demand is equally distributed among all pair of nodes. We discuss in detail a vigorous method to locally update the structure dBBlue using at most Ç ´ÐÓ Òµ communications when a node joins or leaves the network. In most cases the cost of updating the scatternet is actually Ç ´½µ since a node can join or leave without affecting the remaining scatternet. The number of nodes affected when a node joins or leaves the network is always bounded from above by a constant . To facilitate self-routing and easy updating we design a scalable MAC assigning mechanism for piconet which guarantees the packet delivery during scatternet updating. The dBBlue scatternet can be constructed incrementally when the nodes join the network one by one. Previously no method can guarantee all these properties although some methods can achieve some of the properties.,"['scatternet formation', 'single-hop', 'Self-routing Scatternet', 'de Bruijn graph', 'self-routing', 'Bluetooth', 'Bluetooth networks', 'Low Diameter', 'low diameter', 'equal traffic', 'easy updating', 'Network topology', 'ad hoc networks', 'const updating cost', 'Bruijn graph', 'scalable MAC assignment']",266,16
65,66,This text has analyzed the development of E-commerce in some developed countries such as Canada U.S.A. Japan etc and put forward several suggestions on how to set up the system of E-commerce in our country taking the national conditions of our country into account.,"['Statistical methods', 'Development stage', 'Definition of E-commerce', 'China', 'Statistical Survey', 'Survey', 'Measurement', 'E-commerce', 'Implications', 'E-commerce Statistics', 'Statistics', 'Authority']",44,12
66,67,Many authors have recognised the importance of structure in shaping information system IS design and use. Structuration theory has been used in IS research and design to assist with the identification and understanding of the structures in which the IS is situated. From a critical theoretical perspective focusing on the Habermas theory of communicative action a community based child health information system was designed and implemented in a municipality in rural South Africa. The structures which shaped and influenced the design of this IS the restructured health services and social tradition are explored and discussed. From this case study the implications of using IS design as a developmental tool are raised namely the development of a shared understanding the participation of key players and the agreement on joint action.,"['conducive environment', 'the ideal speech situation', 'health information systems', 'communicative action', 'interpretative schemes', 'community monitoring system', 'moral codes or norms', 'Structuration theory', 'duality of structure', 'critical social theory in IS design', 'marginalisation']",129,11
67,68,When failures occur in Internet overlay connections today it is difficult for users to determine the root cause of failure. An overlay connection may require TCP connections between a series of overlay nodes to succeed but accurately determining which of these connections has failed is difficult for users without access to the internal workings of the overlay. Diagnosis using active probing is costly and may be inaccurate if probe packets are filtered or blocked. To address this problem we develop a passive diagnosis approach that infers the most likely cause of failure using a Bayesian network modeling the conditional probability of TCP failures given the IP addresses of the hosts along the overlay path. We collect TCP failure data for 28.3 million TCP connections using data from the new Planetseer overlay monitoring system and train a Bayesian network for the diagnosis of overlay connection failures . We evaluate the accuracy of diagnosis using this Bayesian network on a set of overlay connections generated from observations of CoDeeN traffic patterns and find that our approach can accurately diagnose failures.,"['network address translation', 'TCP overlay path diagnosis', 'active probing for diagnosis', 'clustering', 'Bayesian networks modelling', 'inter-AS TCP failure probabilities', 'NAT', 'Bayesian networks', 'Planetseer overlay monitoring system', 'Planetseer', 'CoDeeN traffic patterns', 'passive diagnosis', 'fault diagnosis', 'TCP overlay connections']",178,14
68,69,Digital Asset Management DAM the management of digital content so that it can be cataloged searched and re-purposed is extremely challenging for organizations that rely on image handling and expect to gain business value from these assets. Metadata plays a crucial role in their management and XML with its inherent support for structural representation is an ideal technology for this. This paper analyzes the capabilities of a native XML database solution via the development of a proof of concept and describes implementation requirements strategy and advantages and disadvantages of this solution.,"['database', 'DAM', 'digital images', 'digital asset management', 'storage and retrieval', 'multimedia', 'keyword search', 'Digital Asset Management', 'metadata', 'XML database', 'proof of concept', 'native XML', 'semi structured data', 'heterogenous data']",91,14
69,70,Web Directories are repositories of Web pages organized in a hierarchy of topics and sub-topics. In this paper we present DirectoryRank  a ranking framework that orders the pages within a given topic according to how informative they are about the topic. Our method works in three steps first it processes Web pages within a topic in order to extract structures that are called lexical chains which are then used for measuring how informative a page is for a particular topic. Then it measures the relative semantic similarity of the pages within a topic. Finally the two metrics are combined for ranking all the pages within a topic before presenting them to the users.,"['information retrieval', 'ranking metric', 'Web Directory', 'lexical chains', 'semantic similarities', 'web directory', 'topic importance', 'topic hierachy', 'semantic similarity', 'PageRank', 'DirectoryRank', 'scoring', 'ranking']",113,13
70,71,In this paper we present a personalized web service discovery and ranking technique for discovering and ranking relevant data-intensive web services. Our first prototype ­ called BASIL ­ supports a personalized view of data-intensive web services through source-biased focus. BASIL provides service discovery and ranking through source-biased probing and source-biased relevance metrics. Concretely the BASIL approach has three unique features 1 It is able to determine in very few interactions whether a target service is relevant to the given source service by probing the target with very precise probes 2 It can evaluate and rank the relevant services discovered based on a set of source-biased relevance metrics and 3 It can identify interesting types of relationships for each source service with respect to other discovered services which can be used as value-added metadata for each service. We also introduce a performance optimization technique called source-biased probing with focal terms to further improve the effectiveness of the basic source-biased service discovery algorithm. The paper concludes with a set of initial experiments showing the effectiveness of the BASIL system.,"['focal terms', 'data-intensive services', 'data-intensive web services', 'query-biased probing', 'web service discovery', 'source-biased probing', 'biased discovery', 'ranking']",177,8
71,72,In visual information retrieval the careful choice of suitable proximity measures is a crucial success factor. The evaluation presented in this paper aims at showing that the distance measures suggested by the MPEG-7 group for the visual descriptors can be beaten by general-purpose measures. Eight visual MPEG-7 descriptors were selected and 38 distance measures implemented. Three media collections were created and assessed performance indicators developed and more than 22500 tests performed. Additionally a quantisation model was developed to be able to use predicate-based distance measures on continuous data as well. The evaluation shows that the distance measures recommended in the MPEG-7-standard are among the best but that other measures perform even better.,"['MPEG-7 descriptors', 'Similarity Measurement', 'human similarity perception', 'distance measure', 'Distance Measurement', 'Content-based Video Retrieval', 'quantisation', 'Visual Information Retrieval', 'Content-based Image Retrieval', 'visual information retrieval', 'MPEG-7', 'Similarity Perception']",112,12
72,73,In the not too distant future Intelligent Creatures robots smart devices smart vehicles smart buildings  etc will share the everyday living environment of human beings. It is important then to analyze the attitudes humans are to adopt for interaction with morphologically different devices based on their appearance and behavior. In particular these devices will become multi-modal interfaces with computers or networks of computers for a large and complex universe of applications. Our results show that children are quickly attached to the word `dog reflecting a conceptualization that robots that look like dogs in particular SONY Aibo are closer to living dogs than they are to other devices. By contrast adults perceive Aibo as having stronger similarities to machines than to dogs reflected by definitions of robot). Illustration of the characteristics structured in the definition of robot are insufficient to convince children Aibo is closer to a machine than to a dog.,"['essence', 'human attitudes', 'agency', 'multi-modal interfaces', 'hci', 'behavioral science', 'zoo-morphological autonomous mobile robots', 'language', 'feelings', 'intelligent creatures', 'perceived attitude', 'robot attributes', 'interaction', 'robots']",151,14
73,74,An ever-increasing amount of information on the Web today is available only through search interfaces the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites. These pages are often referred to as the Hidden Web or the Deep Web. Since there are no static links to the Hidden Web pages search engines cannot discover and index such pages and thus do not return them in the results. However according to recent studies the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users. In this paper we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web. Since the only entry point to a Hidden Web site is a query interface the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site. Here we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically. Our policies proceed iteratively issuing a different query in every iteration . We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising. For instance in one experiment one of our policies downloaded more than 90 of a Hidden Web site that contains 14 million documents  after issuing fewer than 100 queries.,"['accurate language model', 'potential bias', 'Hidden Web crawling', 'keyword queries', 'query selection', 'coverage', 'keyword selection', 'deep web', 'Deep Web crawler', 'crawler', 'adaptive algorithmn', 'hidden web', 'keyword query', 'efficiency', 'adaptive algorithm']",256,15
74,75,Domain-specific languages hold the potential of automating the software development process. Nevertheless the adop-tion of a domain-specific language is hindered by the difficulty of transitioning to different language syntax and employing a separate translator in the software build process. We present a methodology that simplifies the development and deployment of small language extensions in the context of Java. The main language design principle is that of language extension through unobtrusive annotations. The main language implementation idea is to express the language as a generator of customized AspectJ aspects using our Meta-AspectJ tool. The advantages of the approach are twofold. First the tool integrates into an existing software application much as a regular API or library instead of as a language extension. This means that the programmer can remove the language extension at any point and choose to implement the required functionality by hand without needing to rewrite the client code. Second a mature language implementation is easy to achieve with little effort since AspectJ takes care of the low-level issues of interfacing with the base Java language,"['domain-specific language', 'Java', 'Meta-AspectJ', 'language extensions', 'language extension', 'domain-specific languages', 'annotation', 'simplicity']",177,8
75,76,We review a query log of hundreds of millions of queries that constitute the total query traffic for an entire week of a general-purpose commercial web search service. Previously query logs have been studied from a single cumulative view. In contrast our analysis shows changes in popularity and uniqueness of topically categorized queries across the hours of the day. We examine query traffic on an hourly basis by matching it against lists of queries that have been topically pre-categorized by human editors. This represents 13 of the query traffic. We show that query traffic from particular topical categories differs both from the query stream as a whole and from other categories. This analysis provides valuable insight for improving retrieval effectiveness and efficiency. It is also relevant to the development of enhanced query disambiguation routing and caching algorithms.,"['Web Search', 'log analysis', 'query log', 'query stream', 'query traffic', 'Query Log Analysis', 'frequency distribution', 'topical categories']",137,8
76,77,Text categorization is an important research area and has been receiving much attention due to the growth of the on-line information and of Internet. Automated text categorization is generally cast as a multi-class classification problem. Much of previous work focused on binary document classification problems. Support vector machines SVMs excel in binary classification but the elegant theory behind large-margin hyperplane cannot be easily extended to multi-class text classification. In addition the training time and scaling are also important concerns. On the other hand other techniques naturally extensible to handle multi-class classification are generally not as accurate as SVM. This paper presents a simple and efficient solution to multi-class text categorization. Classification problems are first formulated as optimization via discriminant analysis . Text categorization is then cast as the problem of finding coordinate transformations that reflects the inherent similarity from the data. While most of the previous approaches decompose a multiclass classification problem into multiple independent binary classification tasks the proposed approach enables direct multi-class classification. By using Generalized Singular Value Decomposition GSVD a coordinate transformation that reflects the inherent class structure indicated by the generalized singular values is identified. Extensive experiments demonstrate the efficiency and effectiveness of the proposed approach.,"['multi-class classification', 'text categorization', 'GSVD', 'SVMs', 'discriminant analysis', 'GDA', 'Discriminant Analysis', 'Multi-class Text Categorization']",200,8
77,78,Search engines need to evaluate queries extremely fast a challenging task given the vast quantities of data being indexed . A significant proportion of the queries posed to search engines involve phrases. In this paper we consider how phrase queries can be efficiently supported with low disk overheads. Previous research has shown that phrase queries can be rapidly evaluated using nextword indexes but these indexes are twice as large as conventional inverted files. We propose a combination of nextword indexes with inverted files as a solution to this problem. Our experiments show that combined use of an auxiliary nextword index and a conventional inverted file allow evaluation of phrase queries in half the time required to evaluate such queries with an inverted file alone and the space overhead is only 10 of the size of the inverted file. Further time savings are available with only slight increases in disk requirements.,"['evaluation efficiency', 'inverted index', 'nextword index', 'query evaluation', 'phrase query evaluation', 'Indexing', 'phrase query', 'common words', 'index representation', 'stopping']",150,10
78,79,We propose an indexing technique for the fast retrieval of objects in 2D images basedon similarity between their boundary shapes. Our technique is robust in the presence of noise andsupports several important notions of similarity including optimal matches irrespective of variations in orientation and/or position. Our method can also handle size-invariant matches using a normalization technique although optimality is not guaranteedhere. We implementedour method and performed experiments on real hand-written digits data. Our experimental results showedthe superiority of our method comparedto search basedon sequential scanning which is the only obvious competitor. The performance gain of our method increases with any increase in the number or the size of shapes.,"['Similarity queries', 'database', 'Image databases', ' Shape retrieval', 'Similarity retrieval', 'Fourier descriptors', 'queries', 'non textual objects', 'search', 'fingerprint', 'handwriting recognition', 'shape classification', 'indexing techniques', 'efficiency', 'Fourier descriptors']",109,13
79,80,This paper presents the notion of Semantic Associations as complex relationships between resource entities. These relationships capture both a connectivity of entities as well as similarity of entities based on a specific notion of similarity called isomorphism. It formalizes these notions for the RDF data model by introducing a notion of a Property Sequence as a type. In the context of a graph model such as that for RDF Semantic Associations amount to specific certain graph signatures. Specifically they refer to sequences i.e. directed paths here called Property Sequences between entities networks of Property Sequences i.e. undirected paths or subgraphs of isomorphic Property Sequences. The ability to query about the existence of such relationships is fundamental to tasks in analytical domains such as national security and business intelligence where tasks often focus on finding complex yet meaningful and obscured relationships between entities. However support for such queries is lacking in contemporary query systems including those for RDF. This paper discusses how querying for Semantic Associations might be enabled on the Semantic Web through the use of an operator . It also discusses two approaches for processing queries on available persistent RDF stores and memory resident RDF data graphs thereby building on current RDF query languages.,"['analysis', 'semantic web', 'Semantic Associations', 'Rooted Directed Path', 'graph traversals', 'query processing', 'RDF', 'Complex Data Relationships', 'semantic association', 'isomorphism', 'Property Sequence', 'relationship', 'automation', 'Semantic Web Querying', 'AI']",205,15
80,81,With the tremendous growth of system memories memory-resident databases are increasingly becoming important in various domains. Newer memories provide a structured way of storing data in multiple chips with each chip having a bank of memory modules. Current memory-resident databases are yet to take full advantage of the banked storage system which offers a lot of room for performance and energy optimizations. In this paper we identify the implications of a banked memory environment in supporting memory-resident databases and propose hardware memory-directed and software query-directed schemes to reduce the energy consumption of queries executed on these databases. Our results show that high-level query-directed schemes hosted in the query optimizer better utilize the low-power modes in reducing the energy consumption than the respective hardware schemes hosted in the memory controller due to their complete knowledge of query access patterns. We extend this further and propose a query restructuring scheme and a multi-query optimization . Queries are restructured and regrouped based on their table access patterns to maximize the likelihood that data accesses are clustered. This helps increase the inter-access idle times of memory modules which in turn enables a more effective control of their energy behavior. This heuristic is eventually integrated with our hardware optimizations to achieve maximum savings. Our experimental results show that the memory energy reduces by 90 if query restructuring method is applied along with basic energy optimizations over the unoptimized version. The system-wide performance impact of each scheme is also studied simultaneously.,"['database', 'low-power modes', 'multi-query optimization', 'query-directed scheme', 'energy optimization', 'power consumption', 'query-directed energy management', 'query restructuring', 'energy', 'DRAM', 'memory-resident databases', 'banked memory', 'hardware energy scheme']",245,13
81,82,Existing security models require that information of a given security level be prevented from leaking into lower-security information. High-security applications must be demonstrably free of such leaks but such demonstration may require substantial manual analysis. Other authors have argued that the natural way to enforce these models automatically is with information-flow analysis but have not shown this to be practicable for general purpose programming languages in current use. Modern safety-critical systems can contain software components with differing safety integrity levels potentially operating in the same address space. This case poses problems similar to systems with differing security levels failure to show separation of data may require the entire system to be validated at the higher integrity level. In this paper we show how the information flow model enforced by the SPARK Examiner provides support for enforcing these security and safety models. We describe an extension to the SPARK variable annotations which allows the specification of a security or safety level for each state variable and an extension to the SPARK analysis which automatically enforces a given information flow policy on a SPARK program.,"['integrity', 'information flow', 'security level', 'subprogram', 'safety', 'Information flow', 'Dolev-Yao', 'SPARK', 'SPARK Ada', 'Bell-LaPadula', 'security']",183,11
82,83,Emergent self-organization in multi-agent systems appears to contradict the second law of thermodynamics. This paradox has been explained in terms of a coupling between the macro level that hosts self-organization and an apparent reduction in entropy and the micro level where random processes greatly increase entropy). Metaphorically the micro level serves as an entropy sink permitting overall system entropy to increase while sequestering this increase from the interactions where self-organization is desired. We make this metaphor precise by constructing a simple example of pheromone-based coordination defining a way to measure the Shannon entropy at the macro agent and micro pheromone levels and exhibiting an entropy-based view of the coordination.,"['multi-agent system', 'entropy', 'pheromones', 'thermodynamic', 'Pheromones', 'coordination', 'Self-Organization', 'autonomy', 'self-organization', 'Entropy']",109,10
83,84,We propose an entropy-based sensor selection heuristic for localization. Given 1 a prior probability distribution of the target location and 2 the locations and the sensing models of a set of candidate sensors for selection the heuristic selects an informative sensor such that the fusion of the selected sensor observation with the prior target location distribution would yield on average the greatest or nearly the greatest reduction in the entropy of the target location distribution. The heuristic greedily selects one sensor in each step without retrieving any actual sensor observations. The heuristic is also computationally much simpler than the mutual-information-based approaches. The effectiveness of the heuristic is evaluated using localization simulations in which Gaussian sensing models are assumed for simplicity. The heuristic is more effective when the optimal candidate sensor is more informative.,"['mutual information', 'Shannon entropy', 'entropy', 'localization', 'heuristic', 'target localization', 'sensor selection', 'information-directed resource management', 'information fusion', 'wireless sensor networks', 'target tracking']",133,11
84,85,Localized search engines are small-scale systems that index a particular community on the web. They offer several benefits over their large-scale counterparts in that they are relatively inexpensive to build and can provide more precise and complete search capability over their relevant domains. One disadvantage such systems have over large-scale search engines is the lack of global PageRank values. Such information is needed to assess the value of pages in the localized search domain within the context of the web as a whole. In this paper we present well-motivated algorithms to estimate the global PageRank values of a local domain. The algorithms are all highly scalable in that given a local domain of size n they use O(n resources that include computation time bandwidth and storage. We test our methods across a variety of localized domains including site-specific domains and topic-specific domains. We demonstrate that by crawling as few as n or 2n additional pages our methods can give excellent global PageRank estimates.,"['crawling', 'Experimentation', 'localized search engines', 'global PageRank', 'node selection', 'site specific domain', 'Algorithms']",163,7
85,86,Online information services have grown too large for users to navigate without the help of automated tools such as collaborative filtering which makes recommendations to users based on their collective past behavior. While many similarity measures have been proposed and individually evaluated they have not been evaluated relative to each other in a large real-world environment. We present an extensive empirical comparison of six distinct measures of similarity for recommending online communities to members of the Orkut social network. We determine the usefulness of the different recommendations by actually measuring users propensity to visit and join recommended communities. We also examine how the ordering of recommendations influenced user selection as well as interesting social issues that arise in recommending communities within a real social network.,"['recommender system', 'similarity measure', 'social networks', 'online communities', 'Data mining', 'social network', 'community', 'collaborative filtering']",125,8
86,87,We present in this paper the design and an evaluation of a novel interface called the Relation Browser++ RB++ for searching and browsing large information collections. RB++ provides visualized category overviews of an information space and allows dynamic filtering and exploration of the result set by tightly coupling the browsing and searching functions. A user study was conducted to compare the effectiveness efficiency and user satisfaction of completing various types of searching and browsing using the RB++ interface and a traditional form-fillin interface for a video library. An exploration set of tasks was also included to examine the effectiveness of and user satisfaction with the RB++ when applied to a large federal statistics website. The comparison study strongly supported that RB++ was more effective efficient and satisfying for completing data exploration tasks. Based on the results efforts to automatically populate the underlying database using machine learning techniques are underway. Preliminary implementations for two large-scale federal statistical websites have been installed on government servers for internal evaluation.,"['interactive system', 'visualization', 'Interface design', 'searching', 'interaction patterns with interface', 'browsing', 'Faceted category structure', 'information search', 'browse', 'User interface', 'information browsing', 'effectiveness', 'dynamic query', 'facets', 'category overview', 'satisfaction', 'efficiency', 'user interface', 'Browse and Search Interface', 'visual display', 'search', 'Relation Browser++', 'Human Factor', 'Modeling User Interaction', 'RB++', 'Information Storage and Retrieval', 'user satisfaction', 'user study']",166,28
87,88,With the overwhelming volume of online news available today there is an increasing need for automatic techniques to analyze and present news to the user in a meaningful and efficient manner. Previous research focused only on organizing news stories by their topics into a flat hierarchy. We believe viewing a news topic as a flat collection of stories is too restrictive and inefficient for a user to understand the topic quickly. In this work we attempt to capture the rich structure of events and their dependencies in a news topic through our event models. We call the process of recognizing events and their dependencies event threading. We believe our perspective of modeling the structure of a topic is more effective in capturing its semantics than a flat list of on-topic stories. We formally define the novel problem suggest evaluation metrics and present a few techniques for solving the problem. Besides the standard word based features our approaches take into account novel features such as temporal locality of stories for event recognition and time-ordering for capturing dependencies. Our experiments on a manually labeled data sets show that our models effec-tively identify the events and capture dependencies among them.,"['Agglomerative clustering with time decay', 'Intelligent Information Retrieval', 'Complete-Link model', 'Event', 'threading', 'clustering', 'dependency', 'meaningful and efficient analysis and presentation of news', 'News topic modeling', 'Dependency modeling', 'temporal localization of news stories', 'Nearest Parent Model', 'Event Threading', 'information searching', 'Topic detection and tracking', 'Information browsing and organization']",197,16
88,89,In this paper we embed evolutionary computation into statistical learning theory. First we outline the connection between large margin optimization and statistical learning and see why this paradigm is successful for many pattern recognition problems. We then embed evolutionary computation into the most prominent representative of this class of learning methods namely into Support Vector Machines SVM). In contrast to former applications of evolutionary algorithms to SVMs we do not only optimize the method or kernel parameters . We rather use both evolution strategies and particle swarm optimization in order to directly solve the posed constrained optimization problem. Transforming the problem into the Wolfe dual reduces the total runtime and allows the usage of kernel functions. Exploiting the knowledge about this optimization problem leads to a hybrid mutation which further decreases convergence time while classification accuracy is preserved. We will show that evolutionary SVMs are at least as accurate as their quadratic programming counterparts on six real-world benchmark data sets. The evolutionary SVM variants frequently outperform their quadratic programming competitors. Additionally the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite kernel functions and for several possibly competing performance criteria. Track Learning Classifier Systems and other Genetics-Based Machine Learning,"['kernel methods', 'kernels', 'large margin', 'hybrid mutation', 'statistical learning theory', 'evolution strategies', 'Support vector machines', 'evolutionary computation', 'SVM', 'machine learning', 'particle swarms']",207,11
89,90,A goal of human-robot interaction is to allow one user to operate multiple robots simultaneously. In such a scenario the robots provide leverage to the user's attention. The number of such robots that can be operated is called the fan-out of a human-robot team. Robots that have high neglect tolerance and lower interaction time will achieve higher fan-out. We define an equation that relates fan-out to a robot's activity time and its interaction time. We describe how to measure activity time and fan-out. We then use the fan-out equation to compute interaction effort. We can use this interaction effort as a measure of the effectiveness of a human-robot interaction design. We describe experiments that validate the fan-out equation and its use as a metric for improving human-robot interaction.,"['fan-out equation', 'neglect time', 'Human-robot interaction', 'fan-out', 'interaction effort', 'user interface', 'multiple robots', 'interaction time', 'human-robot interaction', 'activity time']",128,10
90,91,We give experimental evidence for the benefits of order-preserving compression in sorting algorithms . While in general any algorithm might benefit from compressed data because of reduced paging requirements we identified two natural candidates that would further benefit from order-preserving compression namely string-oriented sorting algorithms and word-RAM algorithms for keys of bounded length. The word-RAM model has some of the fastest known sorting algorithms in practice. These algorithms are designed for keys of bounded length usually 32 or 64 bits which limits their direct applicability for strings. One possibility is to use an order-preserving compression scheme so that a bounded-key-length algorithm can be applied. For the case of standard algorithms we took what is considered to be the among the fastest nonword RAM string sorting algorithms Fast MKQSort and measured its performance on compressed data. The Fast MKQSort algorithm of Bentley and Sedgewick is optimized to handle text strings. Our experiments show that order-compression techniques results in savings of approximately 15 over the same algorithm on noncompressed data. For the word-RAM we modified Andersson's sorting algorithm to handle variable-length keys. The resulting algorithm is faster than the standard Unix sort by a factor of 1.5X . Last we used an order-preserving scheme that is within a constant additive term of the optimal Hu­Tucker but requires linear time rather than O(m log m where m = | | is the size of the alphabet.,"['Keys of bounded length', 'compression scheme', 'String sorting', 'linear time algorithm', 'Order-preserving compression scheme', 'Sorting algorithms', 'compression ratio', 'data collection', 'random access', 'word-RAM sorting algorithm']",233,10
91,92,FBRAM a new form of dynamic random access memory that greatly accelerates the rendering of Z-buffered primitives is presented . Two key concepts make this acceleration possible. The first is to convert the read-modify-write Z-buffer compare and RGB blend into a single write only operation. The second is to support two levels of rectangularly shaped pixel caches internal to the memory chip. The result is a 10 megabit part that for 3D graphics performs read-modify-write cycles ten times faster than conventional 60 ns VRAMs. A four-way interleaved 100 MHz FBRAM frame buffer can Z-buffer up to 400 million pixels per second. Working FBRAM prototypes have been fabricated.,"['Z-buffer', 'Optimisation', 'Video buffers', 'Rendering rate', 'VRAM', 'parallel graphics algorithms', 'pixel caching', 'Dynamic memory chips', 'Frame buffer', 'Z-buffering', 'Pixel processing', 'rendering', 'Dynamic random access memory', 'Acceleration', 'pixel processing', 'caching', 'Z buffer', 'dynamic memory', 'Pixel Cache', 'memory', 'graphics', 'RGBa blend', 'Caches', '3D graphics hardware', 'Video output bandwidth', 'FBRAM', 'Z-compare', 'DRAM', 'SRAM', '3D graphics', 'FBRam']",107,31
92,94,In this paper we study the problem of finding most topical named entities among all entities in a document which we refer to as focused named entity recognition. We show that these focused named entities are useful for many natural language processing applications such as document summarization  search result ranking and entity detection and tracking. We propose a statistical model for focused named entity recognition by converting it into a classification problem . We then study the impact of various linguistic features and compare a number of classification algorithms. From experiments on an annotated Chinese news corpus we demonstrate that the proposed method can achieve near human-level accuracy.,"['Main topics', 'robust risk minimization', 'Classification methods', 'Linguistic features', 'entity recognition', 'Machine learning approach', 'natural language processing', 'Features', 'text summarization', 'topic identification', 'machine learning', 'Statistical model', 'Natural language processing applications', 'naive Bayes', 'information retrieval', 'sentence extraction', 'automatic summarization', 'ranking', 'classification model', 'decision tree', 'Focused named entity recognition', 'named entities', 'Information retrieval', 'electronic documents', 'Summarization']",108,25
93,95,Starting from P. Sestoft semantics for lazy evaluation we define a new semantics in which normal forms consist of variables pointing to lambdas or constructions. This is in accordance with the more recent changes in the Spineless Tagless G-machine STG machine where constructions only appear in closures lambdas only appeared in closures already in previous versions). We prove the equivalence between the new semantics and Sestoft's. Then a sequence of STG machines are derived formally proving the correctness of each derivation. The last machine consists of a few imperative instructions and its distance to a conventional language is minimal. The paper also discusses the differences between the final machine and the actual STG machine implemented in the Glasgow Haskell Compiler.,"['Abstract machine', 'STG machine', 'abstract machines', 'Functional programming', 'Translation scheme', 'operational semantics', 'Lazy evaluation', 'Haskell compiler', 'Operational semantics', 'Closures', 'Stepwise derivation', 'compiler verification']",120,12
94,96,"Web query classification QC aims to classify Web users queries which are often short and ambiguous into a set of target categories. QC has many applications including page ranking in Web search targeted advertisement in response to queries and personalization. In this paper we present a novel approach for QC that outperforms the winning solution of the ACM KDDCUP 2005 competition whose objective is to classify 800,000 real user queries. In our approach we first build a bridging classifier on an intermediate taxonomy in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the above intermediate taxonomy. A major innovation is that by leveraging the similarity distribution over the intermediate taxonomy we do not need to retrain a new classifier for each new set of target categories and therefore the bridging classifier needs to be trained only once. In addition we introduce category selection as a new method for narrowing down the scope of the intermediate taxonomy based on which we classify the queries. Category selection can improve both efficiency and effectiveness of the online classification. By combining our algorithm with the winning solution of KDDCUP 2005 we made an improvement by 9.7 and 3.8 in terms of precision and F1 respectively compared with the best results of KDDCUP 2005.","['Target categories', 'Mapping functions', 'Category selection', 'Query classification', 'KDDCUP 2005', 'Taxonomy', 'Category Selection', 'Intermediate categories', 'Matching approaches', 'Ensemble classifier', 'Web Query Classification', 'Bridging classifier', 'Search engine', 'Bridging Classifier', 'Similarity distribution', 'Query enrichment']",222,16
95,97,A collaborative crawler is a group of crawling nodes in which each crawling node is responsible for a specific portion of the web. We study the problem of collecting geographically aware pages using collaborative crawling strategies. We first propose several collaborative crawling strategies for the geographically focused crawling whose goal is to collect web pages about specified geographic locations by considering features like URL address of page content of page extended anchor text of link and others. Later we propose various evaluation criteria to qualify the performance of such crawling strategies. Finally we experimentally study our crawling strategies by crawling the real web data showing that some of our crawling strategies greatly outperform the simple URL-hash based partition collaborative crawling in which the crawling assignments are determined according to the hash-value computation over URLs. More precisely features like URL address of page and extended anchor text of link are shown to yield the best overall performance for the geographically focused crawling.,"['crawling strategies', 'Crawling strategies', 'Geo-coverage', 'Geographically focused crawling', 'geographically focused crawling', 'Full Content based', 'Hash based collaboration', 'Search engine', 'Geo-focus', 'IP address based', 'Problems of aliasing and ambiguity', 'collaborative crawling', 'Normalization and Disambiguation of City Names', 'geographical nodes', 'Geographic Locality', 'Quality issue', 'geographic entities', 'Evaluation criteria', 'Geographic locality', 'Scalability', 'Focused crawler', 'Anchor text', 'Search localization', 'URL based', 'pattern matching', 'Collaborative crawler', 'Extracted URL', 'Collaborative crawling']",161,28
96,98,In this paper we consider implementations of embedded 3D graphics and provide evidence indicating that 3D benchmarks employed for desktop computers are not suitable for mobile environments. Consequently we present GraalBench a set of 3D graphics workloads representative for contemporary and emerging mobile devices . In addition we present detailed simulation results for a typical rasterization pipeline. The results show that the proposed benchmarks use only a part of the resources offered by current 3D graphics libraries. For instance while each benchmark uses the texturing unit for more than 70 of the generated fragments the alpha unit is employed for less than 13 of the fragments. The Fog unit was used for 84 of the fragments by one benchmark but the other benchmarks did not use it at all. Our experiments on the proposed suite suggest that the texturing depth and blending units should be implemented in hardware while for instance the dithering unit may be omitted from a hardware implementation. Finally we discuss the architectural implications of the obtained results for hardware implementations.,"['OpenGL', 'mobile phones', 'Graalbench', 'performance', '3D graphics applications', 'triangles', 'workload characterization', 'bottlenecks', 'statistics', 'architecture', 'pipeline', 'openGL', 'embedded 3D graphics architectures', 'accelerators', 'unit', 'Mobile devices', 'measurement', '3D graphics benchmarking', 'API', 'embedded systems', 'mechanism', '3D graphics benchmarks', 'transform and lighting', 'GraalBench', 'embedded 3D graphics', 'benchmark', 'rasterization', 'real-time', 'workload', 'Mobile environments', '3D graphics']",174,31
97,99,Vertical handoff is a switching process between heterogeneous wireless networks in a hybrid 3G/WLAN network. Vertical handoffs fromWLAN to 3G network often fail due to the abrupt degrade of the WLAN signal strength in the transition areas. In this paper a Handoff Trigger Table is introduced to improve the performance of vertical handoff. Based on this table a proactive handoff scheme is proposed. Simulation results show that with the proposed scheme the vertical handoff decisions will be more efficient so that dropping probability can be decreased dramatically.,"['3G', 'vertical handoff', 'Wireless communications', 'wireles networks', 'Handoff trigger table', 'cellular network', 'integrated networks', 'WLAN']",87,8
98,100,In the present paper we will describe the design and implementation of a real-time distributed system of Web crawling running on a cluster of machines. The system crawls several thousands of pages every second includes a high-performance fault manager is platform independent and is able to adapt transparently to a wide range of configurations without incurring additional hardware expenditure. We will then provide details of the system architecture and describe the technical choices for very high performance crawling. Finally we will discuss the experimental results obtained comparing them with other documented systems.,"['dominos process', 'URL caching', 'targeted crawling', 'Hierarchical Cooperation', 'Erlang development kit', 'Dominos distributed database', 'breadth-first crawling', 'fault tolerance', 'high availability', 'limiting disk access', 'Document fingerprint', 'Dominos nodes', 'deep web crawling', 'real-time distributed system', 'high performance crawling system', 'Dominos Generic server', 'High Availability System', 'repetitive crawling', 'Random walks and sampling', 'maintaiability and configurability', 'Breadth first crawling', 'Dominos RPC concurrent', 'Deep web crawling', 'Web Crawler', 'crawling system']",92,25
99,101,Many exploration and manipulation tasks benefit from a coherent integration of multiple views onto complex information spaces. This paper proposes the concept of Illustrative Shadows for a tight integration of interactive 3D graphics and schematic depictions using the shadow metaphor. The shadow metaphor provides an intuitive visual link between 3D and 2D visualizations integrating the different displays into one combined information display. Users interactively explore spatial relations in realistic shaded virtual models while functional correlations and additional textual information are presented on additional projection layers using a semantic network approach. Manipulations of one visualization immediately influence the others resulting in an in-formationally and perceptibly coherent presentation.,"['Information visualization', 'Spreading activation']",106,2
100,102,The current boom of the Web is associated with the revenues originated from on-line advertising. While search-based advertising is dominant the association of ads with a Web page during user navigation is becoming increasingly important . In this work we study the problem of associating ads with a Web page referred to as content-targeted advertising  from a computer science perspective. We assume that we have access to the text of the Web page the keywords declared by an advertiser and a text associated with the advertiser's business. Using no other information and operating in fully automatic fashion we propose ten strategies for solving the problem and evaluate their effectiveness. Our methods indicate that a matching strategy that takes into account the semantics of the problem referred to as AAK for ads and keywords can yield gains in average precision figures of 60 compared to a trivial vector-based strategy. Further a more sophisticated impedance coupling strategy which expands the text of the Web page to reduce vocabulary impedance with regard to an advertisement can yield extra gains in average precision of 50%. These are first results . They suggest that great accuracy in content-targeted advertising can be attained with appropriate algorithms.,"['', 'matching', 'content-targeted advertising', 'triggering page', 'Web', 'Bayesian networks', 'kNN', 'impedance coupling', 'Advertising', 'advertisements']",200,10
101,103,The recently promulgated IT model curriculum contains IT fundamentals as one of its knowledge areas. It is intended to give students a broad understanding of 1 the IT profession and the skills that students must develop to become successful IT professionals and 2 the academic discipline of IT and its relationship to other disciplines. As currently defined the IT fundamentals knowledge area requires 33 lecture hours to complete. The model curriculum recommends that the material relevant to the IT fundamentals knowledge area be offered early in the curriculum for example in an introduction to IT course however many institutions will have to include additional material in an introductory IT course. For example the Introduction of IT course at Georgia Southern University is used to introduce students to the available second disciplines an important part of the Georgia Southern IT curriculum aimed at providing students with in-depth knowledge of an IT application domain some productivity tools and SQL. For many programs there may be too much material in an introductory IT course. This paper describes how Georgia Southern University resolved this dilemma.,"['IT Fundamentals Knowledge Area', 'IT Model Curriculum']",181,2
102,104,Information retrieval systems e.g. web search engines are critical for overcoming information overload. A major deficiency of existing retrieval systems is that they generally lack user modeling and are not adaptive to individual users resulting in inherently non-optimal retrieval performance. For example a tourist and a programmer may use the same word java to search for different information but the current search systems would return the same results. In this paper we study how to infer a user's interest from the user's search context and use the inferred implicit user model for personalized search . We present a decision theoretic framework and develop techniques for implicit user modeling in information retrieval. We develop an intelligent client-side web search agent UCAIR that can perform eager implicit feedback e.g. query expansion based on previous queries and immediate result reranking based on clickthrough information. Experiments on web search show that our search agent can improve search accuracy over the popular Google search engine.,"['retrieval accuracy', 'information retrieval systems', 'interactive retrieval', 'UCAIR', 'clickthrough information', 'user modelling', 'personalized search', 'user model', 'implicit feedback']",160,9
103,105,Nearest neighbour NN searches and k nearest neighbour k-NN searches are widely used in pattern recognition and image retrieval. An NN k-NN search finds the closest object closest k objects to a query object. Although the definition of the distance between objects depends on applications its computation is generally complicated and time-consuming. It is therefore important to reduce the number of distance computations. TLAESA Tree Linear Approximating and Eliminating Search Algorithm is one of the fastest algorithms for NN searches. This method reduces distance computations by using a branch and bound algorithm. In this paper we improve both the data structure and the search algorithm of TLAESA. The proposed method greatly reduces the number of distance computations. Moreover we extend the improved method to an approximation search algorithm which ensures the quality of solutions. Experimental results show that the proposed method is efficient and finds an approximate solution with a very low error rate.,"['TLAESA', 'Approximation Search', 'Distance Computaion', 'k Nearest Neighbour Search', 'Nearest Neighbour Search']",154,5
104,106,Programs in embedded languages contain invariants that are not automatically detected or enforced by their host language. We show how to use macros to easily implement partial evaluation of embedded interpreters in order to capture invariants encoded in embedded programs and render them explicit in the terms of their host language . We demonstrate the effectiveness of this technique in improving the results of a value flow analysis.,"['interpreter', 'set-based analysis', 'embedded language', 'Partial evaluation', 'embedded languages', 'value flow analysis', 'macros', 'Scheme', 'regular expression', 'flow analysis', 'partial evaluation']",68,11
105,107,Many real life sequence databases grow incrementally. It is undesirable to mine sequential patterns from scratch each time when a small set of sequences grow or when some new sequences are added into the database. Incremental algorithm should be developed for sequential pattern mining so that mining can be adapted to incremental database updates . However it is nontrivial to mine sequential patterns incrementally especially when the existing sequences grow incrementally because such growth may lead to the generation of many new patterns due to the interactions of the growing subsequences with the original ones. In this study we develop an efficient algorithm IncSpan for incremental mining of sequential patterns by exploring some interesting properties. Our performance study shows that IncSpan outperforms some previously proposed incremental algorithms as well as a non-incremental one with a wide margin.,"['frequent itemsets', 'database updates', 'buffering pattern', 'optimization', 'sequence database', 'incremental mining', 'sequential pattern', 'reverse pattern matching', 'buffering patterns', 'shared projection']",137,10
106,108,A technical infrastructure for storing querying and managing RDF data is a key element in the current semantic web development. Systems like Jena Sesame or the ICS-FORTH RDF Suite are widely used for building semantic web applications. Currently none of these systems supports the integrated querying of distributed RDF repositories. We consider this a major shortcoming since the semantic web is distributed by nature. In this paper we present an architecture for querying distributed RDF repositories by extending the existing Sesame system. We discuss the implications of our architecture and propose an index structure as well as algorithms for query processing and optimization in such a distributed context.,"['distributed architecture', 'index structure', 'semantic web', 'external sources', 'Index Structures', 'join ordering problem', 'Optimization', 'RDF', 'query optimization', 'infrastructure', 'repositories', 'RDF Querying']",108,12
107,109,Although most time-series data mining research has concentrated on providing solutions for a single distance function in this work we motivate the need for a single index structure that can support multiple distance measures. Our specific area of interest is the efficient retrieval and analysis of trajectory similarities. Trajectory datasets are very common in environmental applications mobility experiments video surveillance and are especially important for the discovery of certain biological patterns. Our primary similarity measure is based on the Longest Common Subsequence LCSS model that offers enhanced robustness particularly for noisy data which are encountered very often in real world applications . However our index is able to accommodate other distance measures as well including the ubiquitous Euclidean distance and the increasingly popular Dynamic Time Warping DTW). While other researchers have advocated one or other of these similarity measures a major contribution of our work is the ability to support all these measures without the need to restructure the index. Our framework guarantees no false dismissals and can also be tailored to provide much faster response time at the expense of slightly reduced precision/recall. The experimental results demonstrate that our index can help speed-up the computation of expensive similarity measures such as the LCSS and the DTW.,"['Longest Common Subsequence', 'Longest Common Subsequence (LCSS)', 'trajectory', 'Dynamic Time Warping (DTW)', 'indexing', 'Dynamic Time Warping', 'similarity', 'measure', 'distance function', 'Trajectories']",207,10
108,110,Web pages and resources in general can be characterized according to their geographical locality. For example a web page with general information about wildflowers could be considered a global page likely to be of interest to a ge-ographically broad audience. In contrast a web page with listings on houses for sale in a specific city could be regarded as a local page likely to be of interest only to an audience in a relatively narrow region. Similarly some search engine queries implicitly target global pages while other queries are after local pages. For example the best results for query wildflowers are probably global pages about wildflowers such as the one discussed above. However local pages that are relevant to say San Francisco are likely to be good matches for a query houses for sale that was issued by a San Francisco resident or by somebody moving to that city. Unfortunately search engines do not analyze the geographical locality of queries and users and hence often produce sub-optimal results. Thus query wildflowers  might return pages that discuss wildflowers in specific U.S. states and not general information about wildflowers while query houses for sale might return pages with real estate listings for locations other than that of interest to the person who issued the query. Deciding whether an unseen query should produce mostly local or global pages--without placing this burden on the search engine users--is an important and challenging problem because queries are often ambiguous or underspecify the information they are after. In this paper we address this problem by first defining how to categorize queries according to their often implicit geographical locality. We then introduce several alternatives for automatically and efficiently categorizing queries in our scheme using a variety of state-of-the-art machine learning tools. We report a thorough evaluation of our classifiers using a large sample of queries from a real web search engine and conclude by discussing how our query categorization approach can help improve query result quality.,"['information retrieval', 'local page', 'query classification', 'web search', 'geographical locality', 'search engine', 'web queries', 'query categorization / query classification', 'search engines', 'global page', 'categorization scheme', 'query modification']",328,12
109,111,"Participation in social networking sites has dramatically increased in recent years. Services such as Friendster Tribe or the Facebook allow millions of individuals to create online profiles and share personal information with vast networks of friends  and often unknown numbers of strangers. In this paper we study patterns of information revelation in online social networks and their privacy implications. We analyze the online behavior of more than 4,000 Carnegie Mellon University students who have joined a popular social networking site catered to colleges. We evaluate the amount of information they disclose and study their usage of the site's privacy settings. We highlight potential attacks on various aspects of their privacy and we show that only a minimal percentage of users changes the highly permeable privacy preferences.","['information relevation', 'privacy', 're-identification', 'Online privacy', 'information revelation', 'online behavior', 'college', 'social network theory', 'privacy perference', 'online social networking', 'stalking', 'facebook', 'social networking sites', 'data visibility', 'privacy risk']",126,15
110,112,Topic distillation is the process of finding authoritative Web pages and comprehensive hubs which reciprocally endorse each other and are relevant to a given query. Hyperlink-based topic distillation has been traditionally applied to a macroscopic Web model where documents are nodes in a directed graph and hyperlinks are edges. Macroscopic models miss valuable clues such as banners navigation panels  and template-based inclusions which are embedded in HTML pages using markup tags. Consequently results of macroscopic distillation algorithms have been deteriorating in quality as Web pages are becoming more complex. We propose a uniform fine-grained model for the Web in which pages are represented by their tag trees also called their Document Object Models or DOMs and these DOM trees are interconnected by ordinary hyperlinks. Surprisingly macroscopic distillation algorithms do not work in the fine-grained scenario. We present a new algorithm suitable for the fine-grained model. It can dis-aggregate hubs into coherent regions by segmenting their DOMtrees. M utual endorsement between hubs and authorities involve these regions  rather than single nodes representing complete hubs. Anecdotes and measurements using a 28-query 366000-document benchmark suite used in earlier topic distillation research reveal two benefits from the new algorithm distillation quality improves and a by-product of distillation is the ability to extract relevant snippets from hubs which are only partially relevant to the query.,"['Document Object Model', 'segmentation', 'microscopic distillation', 'XML', 'Google', 'PageRank algorithm', 'hyperlink', 'Topic distillation', 'topic distillation', 'link localization', 'hub fragmentation', 'DOM', 'Minimum Description Length principle', 'HITS', 'text analysis']",220,15
111,113,In this paper we present the context of the work of the Curriculum Committee on IT2005 the IT curriculum volume described in the Overview Draft document of the Joint Task Force for Computing Curriculum 2004. We also provide a brief introduction to the history and work of the Information Assurance Education community. These two perspectives provide the foundation for the main thrust of the paper which is a description of the Information Assurance and Security IAS component of the IT2005 document. Finally we end the paper with an example of how IAS is being implemented at BYU as a pervasive theme that is woven throughout the curriculum and conclude with some observations about the first year's experience.,"['Information Assurance', 'IT', 'Committee on National Security Systems', 'Training standards', 'Information assurance', 'SIGITE Curriculum committee', 'NIETP Program', 'Pervasive Themes', 'In-service training development', 'BYU curriculum', 'Information Technology', 'IT2005', 'IAS', 'Security Knowledge', 'IA', 'CITC-1', 'CC2005', 'Education', 'IT2005 volume']",117,19
112,114,Perceptual user interfaces PUIs are an important part of ubiquitous computing. Creating such interfaces is difficult because of the image and signal processing knowledge required for creating classifiers. We propose an interactive machine-learning IML model that allows users to train classify/view and correct the classifications. The concept and implementation details of IML are discussed and contrasted with classical machine learning models. Evaluations of two algorithms are also presented. We also briefly describe Image Processing with Crayons Crayons which is a tool for creating new camera-based interfaces using a simple painting metaphor. The Crayons tool embodies our notions of interactive machine learning.,"['image processing', 'perceptive user interfaces', 'Classical machine learning', 'Crayons prototype', 'Crayons design process', 'classification', 'Image processing with crayons', 'Perceptual user iinterfaces', 'Interactive machine learning', 'Classification design loop', 'Predict correct behaviour', 'Machine learning', 'Perceptual interface', 'interaction', 'image/pixel classifier']",101,15
113,115,The paper presents a method for pruning frequent itemsets based on background knowledge represented by a Bayesian network. The interestingness of an itemset is defined as the absolute difference between its support estimated from data and from the Bayesian network. Efficient algorithms are presented for finding interestingness of a collection of frequent itemsets and for finding all attribute sets with a given minimum interestingness. Practical usefulness of the algorithms and their efficiency have been verified experimentally.,"['background knowledge', 'frequent itemsets', 'emerging pattern', 'Bayesian network', 'interestingness', 'association rules', 'association rule', 'frequent itemset']",76,8
114,116,Abstract. The emergence of several radio technologies such as Bluetooth and IEEE 802.11 operating in the 2.4 GHz unlicensed ISM frequency band may lead to signal interference and result in significant performance degradation when devices are colocated in the same environment. The main goal of this paper is to evaluate the effect of mutual interference on the performance of Bluetooth and IEEE 802.11b systems. We develop a simulation framework for modeling interference based on detailed MAC and PHY models. First we use a simple simulation scenario to highlight the effects of parameters such as transmission power offered load and traffic type. We then turn to more complex scenarios involving multiple Bluetooth piconets and WLAN devices.,"['topology', 'evaluation', 'interference', 'performance degradation', 'offered load', 'tranmission power', 'simulation framework', 'IEEE 802.11b', 'WPANs', 'hop rate', 'packet loss', 'WLAN', 'Bluetooth']",115,13
115,117,What makes a peripheral or ambient display more effective at presenting awareness information than another  Presently little is known in this regard and techniques for evaluating these types of displays are just beginning to be developed. In this article we focus on one aspect of a peripheral display's effectiveness-its ability to communicate information at a glance. We conducted an evaluation of the InfoCanvas a peripheral display that conveys awareness information graphically as a form of information art by assessing how well people recall information when it is presented for a brief period of time. We compare performance of the InfoCanvas to two other electronic information displays  a Web portal style and a text-based display when each display was viewed for a short period of time. We found that participants noted and recalled significantly more information when presented by the InfoCanvas than by either of the other displays despite having to learn the additional graphical representations employed by the InfoCanvas.,"['Web portal-like display', 'empirical evaluation', 'evaluation', 'Peripheral display', 'information conveyance', 'graphical representation', 'information visualization', 'peripheral display', 'awareness information', 'ambient display', 'text-based display', 'information recall', 'InfoCanvas display']",159,13
116,118,We propose an In-Network Data-Centric Storage INDCS scheme for answering ad-hoc queries in sensor networks. Previously proposed In-Network Storage INS schemes suffered from Storage Hot-Spots that are formed if either the sensors locations are not uniformly distributed over the coverage area or the distribution of sensor readings is not uniform over the range of possible reading values. Our K-D tree based Data-Centric Storage KDDCS scheme maintains the invariant that the storage of events is distributed reasonably uniformly among the sensors. KDDCS is composed of a set of distributed algorithms whose running time is within a poly-log factor of the diameter of the network. The number of messages any sensor has to send as well as the bits in those messages is poly-logarithmic in the number of sensors. Load balancing in KDDCS is based on defining and distributively solving a theoretical problem that we call the Weighted Split Median problem . In addition to analytical bounds on KDDCS individual algorithms  we provide experimental evidence of our scheme's general efficiency as well as its ability to avoid the formation of storage hot-spots of various sizes unlike all previous INDCS schemes.,"['Distributed Algorithms', 'sensor network', 'Sensor Network', 'weighted split median problem', 'storage hot-spots', 'energy saving', 'Power-Aware', 'KDDCS', 'ad-hoc queries', 'quality of data (QoD)', 'data persistence', 'routing algorithm', 'DIM']",188,13
117,119,Topic tracking is complicated when the stories in the stream occur in multiple languages. Typically researchers have trained only English topic models because the training stories have been provided in English. In tracking non-English test stories are then machine translated into English to compare them with the topic models. We propose a native language hypothesis stating that comparisons would be more effective in the original language of the story. We first test and support the hypothesis for story link detection. For topic tracking the hypothesis implies that it should be preferable to build separate language-specific topic models for each language in the stream. We compare different methods of incrementally building such native language topic models.,"['', 'crosslingual', 'Arabic', 'multilingual topic tracking', 'multilingual', 'classification', 'story link', 'native language hypothesis', 'machine translation', 'native topic models', 'similarity', 'TDT', 'topic models', 'topic tracking']",115,14
118,120,Backup of websites is often not considered until after a catastrophic event has occurred to either the website or its webmaster. We introduce lazy preservation ­ digital preservation performed as a result of the normal operation of web crawlers and caches. Lazy preservation is especially suitable for third parties for example a teacher reconstructing a missing website used in previous classes. We evaluate the effectiveness of lazy preservation by reconstructing 24 websites of varying sizes and composition using Warrick a web-repository crawler. Because of varying levels of completeness in any one repository our reconstructions sampled from four different web repositories Google 44 MSN 30 Internet Archive 19 and Yahoo 7%). We also measured the time required for web resources to be discovered and cached 10-103 days as well as how long they remained in cache after deletion 7-61 days).,"['recovery', 'caching', 'reconstruction', 'digital preservation', 'crawling', 'search engine', 'Search engines (SEs)', 'cached resources', 'web repositories', 'lazy preservation']",139,10
119,121,This paper considers the problem of using Support Vector Machines SVMs to learn concepts from large scale imbalanced data sets. The objective of this paper is twofold. Firstly we investigate the effects of large scale and imbalance on SVMs. We highlight the role of linear non-separability in this problem. Secondly we develop a both practical and theoretical guaranteed meta-algorithm to handle the trouble of scale and imbalance. The approach is named Support Cluster Machines SCMs). It incorporates the informative and the representative under-sampling mechanisms to speedup the training procedure. The SCMs differs from the previous similar ideas in two ways a the theoretical foundation has been provided and b the clustering is performed in the feature space rather than in the input space. The theoretical analysis not only provides justification  but also guides the technical choices of the proposed approach. Finally experiments on both the synthetic and the TRECVID data are carried out. The results support the previous analysis and show that the SCMs are efficient and effective while dealing with large scale imbalanced data sets.,"['Clustering', 'TRECVID', 'Support Vector Machines', 'Support Vector Machines (SVMs)', 'support cluster machines (SCMs)', 'kernel k-means', 'imbalanced data', 'meta-algorithm', 'concept modelling', 'shrinking techniques', 'Large Scale', 'Concept Modelling', 'Imbalance', 'large scale data', 'Kernel k-means', 'clusters']",176,16
120,122,This paper studies the problem of automatic acquisition of the query languages supported by a Web information resource . We describe a system that automatically probes the search interface of a resource with a set of test queries and analyses the returned pages to recognize supported query operators. The automatic acquisition assumes the availability of the number of matches the resource returns for a submitted query. The match numbers are used to train a learning system and to generate classification rules that recognize the query operators supported by a provider and their syntactic encodings. These classification rules are employed during the automatic probing of new providers to determine query operators they support. We report on results of experiments with a set of real Web resources.,"['web resources', 'learning', 'Hidden Web', 'search interface', 'search engine', 'machine learning', 'hidden web', 'query languages', 'web interfaces', 'query operators', 'automatic acquisition']",125,11
121,123,Clustering algorithms typically operate on a feature vector representation of the data and find clusters that are compact with respect to an assumed dis)similarity measure between the data points in feature space. This makes the type of clusters identified highly dependent on the assumed similarity measure. Building on recent work in this area we formally define a class of spatially varying dissimilarity measures and propose algorithms to learn the dissimilarity measure automatically from the data. The idea is to identify clusters that are compact with respect to the unknown spatially varying dissimilarity measure. Our experiments show that the proposed algorithms are more stable and achieve better accuracy on various textual data sets when compared with similar algorithms proposed in the literature.,"['Clustering', 'dissimilarity measure', 'clustering', 'feature weighting', 'spatially varying dissimilarity (SVaD)', 'Learning Dissimilarity Measures']",121,6
122,124,Kernel machines have been shown as the state-of-the-art learning techniques for classification. In this paper we propose a novel general framework of learning the Unified Kernel Machines UKM from both labeled and unlabeled data. Our proposed framework integrates supervised learning semi-supervised kernel learning and active learning in a unified solution. In the suggested framework we particularly focus our attention on designing a new semi-supervised kernel learning method i.e. Spectral Kernel Learning SKL which is built on the principles of kernel target alignment and unsupervised kernel design. Our algorithm is related to an equivalent quadratic programming problem that can be efficiently solved. Empirical results have shown that our method is more effective and robust to learn the semi-supervised kernels than traditional approaches. Based on the framework we present a specific paradigm of unified kernel machines with respect to Kernel Logistic Regresions KLR i.e. Unified Kernel Logistic Regression UKLR). We evaluate our proposed UKLR classification scheme in comparison with traditional solutions. The promising results show that our proposed UKLR paradigm is more effective than the traditional classification approaches.,"['supervised learning', 'unified kernel machine(UKM)', 'data mining', 'Semi-Supervised Learning', 'classification', 'Spectral Kernel Learning', 'framework', 'spectral kernel learning (SKL)', 'Active Learning', 'Kernel Logistic Regressions', 'Classification', 'semi-supervised kernel learning', 'Unsuper-vised Kernel Design', 'active learning', 'Supervised Learning', 'Kernel Machines']",176,16
123,125,A physically compact low cost high performance 3D graphics accelerator is presented. It supports shaded rendering of triangles and antialiased lines into a double-buffered 24-bit true color frame buffer with a 24-bit Z-buffer. Nearly the only chips used besides standard memory parts are 11 ASICs of four types). Special geometry data reformatting hardware on one ASIC greatly speeds and simplifies the data input pipeline. Floating-point performance is enhanced by another ASIC a custom graphics microprocessor with specialized graphics instructions and features. Screen primitive rasterization is carried out in parallel by five drawing ASICs employing a new partitioning of the back-end rendering task. For typical rendering cases the only system performance bottleneck is that intrinsically imposed by VRAM.,"['video output', '3D graphics hardware', 'floating-point microprocessors', 'geometry data', 'floating point processing', 'general graphics processing', '3D shaded graphics', 'rendering', 'antialiased lines', 'low cost', 'parallel algorithms', 'input processing', 'small physical size', 'gouraud shading', 'screen space rendering', 'parallel graphics algorithms']",117,16
124,126,Data dissemination through wireless channels for broadcasting information to consumers is becoming quite common. Many dissemination schemes have been proposed but most of them push data to wireless channels for general consumption. Push based broadcast 1 is essentially asymmetric i.e. the volume of data being higher from the server to the users than from the users back to the server. Push based scheme requires some indexing which indicates when the data will be broadcast and its position in the broadcast. Access latency and tuning time are the two main parameters which may be used to evaluate an indexing scheme. Two of the important indexing schemes proposed earlier were tree based and the exponential indexing schemes. None of these schemes were able to address the requirements of location dependent data LDD which is highly desirable feature of data dissemination. In this paper we discuss the broadcast of LDD in our project DAta in Your Space DAYS and propose a scheme for indexing LDD. We argue that this scheme when applied to LDD significantly improves performance in terms of tuning time over the above mentioned schemes. We prove our argument with the help of simulation results.,"['push based architecture', 'indexing', 'wireless', 'location dependent data', 'Wireless data broadcast', 'location based services', 'data dissemination', 'mapping function', 'broadcast', 'data staging', 'access efficiency', 'indexing scheme', 'energy conservation', 'containment']",194,14
125,127,Bagging frequently improves the predictive performance of a model. An online version has recently been introduced which attempts to gain the benefits of an online algorithm while approximating regular bagging. However regular online bagging is an approximation to its batch counterpart and so is not lossless with respect to the bagging operation. By operating under the Bayesian paradigm we introduce an online Bayesian version of bagging which is exactly equivalent to the batch Bayesian version and thus when combined with a lossless learning algorithm gives a completely lossless online bagging algorithm. We also note that the Bayesian formulation resolves a theoretical problem with bagging produces less variability in its estimates and can improve predictive performance for smaller data sets.,"['online bagging', 'Bayesian bagging', 'bagging', 'classification', 'Dirichlet Distribution', 'Classification Tree', 'mean-squared prediction error', 'lossless learning algorithm', 'bootstrap', 'Bayesian Bootstrap']",119,10
126,128,For hardware accelerated rendering photon mapping is especially useful for simulating caustic lighting effects on non-Lambertian surfaces. However an efficient hardware algorithm for the computation of the k nearest neighbours to a sample point is required. Existing algorithms are often based on recursive spatial subdivision techniques such as kd-trees. However hardware implementation of a tree-based algorithm would have a high latency or would require a large cache to avoid this latency on average. We present a neighbourhood-preserving hashing algorithm that is low-latency and has sub-linear access time. This algorithm is more amenable to fine-scale parallelism than tree-based recursive spatial subdivision and maps well onto coherent block-oriented pipelined memory access. These properties make the algorithm suitable for implementation using future programmable fragment shaders with only one stage of dependent texturing.,"['hashing techniques', 'block hashing (BH)', 'photon mapping', 'kNN', 'AkNN', 'accelerator']",129,6
127,129,In this paper we study the problem of assigning unit-size tasks to related machines when only limited online information is provided to each task. This is a general framework whose special cases are the classical multiple-choice games for the assignment of unit-size tasks to identical machines. The latter case was the subject of intensive research for the last decade. The problem is intriguing in the sense that the natural extensions of the greedy oblivious schedulers which are known to achieve near-optimal performance in the case of identical machines are proved to perform quite poorly in the case of the related machines. In this work we present a rather surprising lower bound stating that any oblivious scheduler that assigns an arbitrary number of tasks to n related machines would need log n loglog n polls of machine loads per task in order to achieve a constant competitive ratio versus the optimum offline assignment of the same input sequence to these machines . On the other hand we prove that the missing information for an oblivious scheduler to perform almost optimally  is the amount of tasks to be inserted into the system. In particular we provide an oblivious scheduler that only uses O(loglog n polls along with the additional information of the size of the input sequence in order to achieve a constant competitive ratio vs. the optimum offline assignment . The philosophy of this scheduler is based on an interesting exploitation of the slowfit concept 1 5 3 for a survey see 6 9 16 for the assignment of the tasks to the related machines despite the restrictions on the provided online information in combination with a layered in-This work has been partially supported by the IST Program of the European Union under contract number IST-1999 14186  ALCOM-FT,"['Limited Information', 'Related Machines', 'related machines', 'HOPS', 'input sequence', 'Online Load Balancing', 'unit-size task', 'lower bounds', 'online information', 'scheduling', 'oblivious scheduler']",297,11
128,130,This paper describes ongoing research into the application of machine learning techniques for improving access to governmental information in complex digital libraries. Under the auspices of the GovStat Project our goal is to identify a small number of semantically valid concepts that adequately spans the intellectual domain of a collection. The goal of this discovery is twofold. First we desire a practical aid for information architects. Second automatically derived document-concept relationships are a necessary precondition for real-world deployment of many dynamic interfaces. The current study compares concept learning strategies based on three document representations keywords titles and full-text. In statistical and user-based studies human-created keywords provide significant improvements in concept learning over both title-only and full-text representations.,"['document representation', 'information architecture', 'Machine Learning', 'clustering', 'document classification', 'Information Architecture', 'Interface Design', 'relational browser', 'digital libraries', 'BLS', 'topic discovery']",117,11
129,131,The development of microarray technology has supplied a large volume of data to many fields. In particular it has been applied to prediction and diagnosis of cancer so that it expectedly helps us to exactly predict and diagnose cancer. To precisely classify cancer we have to select genes related to cancer because extracted genes from microarray have many noises. In this paper we attempt to explore many features and classifiers using three benchmark datasets to systematically evaluate the performances of the feature selection methods and machine learning classifiers. Three benchmark datasets are Leukemia cancer dataset Colon cancer dataset and Lymphoma cancer data set. Pearson's and Spearman's correlation coefficients Euclidean distance cosine coefficient information gain mutual information and signal to noise ratio have been used for feature selection. Multi-layer perceptron k-nearest neighbour support vector machine and structure adaptive self­organizing map have been used for classification. Also we have combined the classifiers to improve the performance of classification. Experimental results show that the ensemble with several basis classifiers produces the best recognition rate on the benchmark dataset.,"['gene expression profile', 'KNN', 'classification', 'feature selection', 'ensemble classifier', 'SVM', 'Biological data mining', 'MLP', 'SASOM']",175,9
130,132,Machine learning and data mining have found a multitude of successful applications in microarray analysis with gene clustering and classification of tissue samples being widely cited examples. Low-level microarray analysis ­ often associated with the pre-processing stage within the microarray life-cycle ­ has increasingly become an area of active research traditionally involving techniques from classical statistics. This paper explores opportunities for the application of machine learning and data mining methods to several important low-level microarray analysis problems monitoring gene expression transcript discovery genotyping and resequencing . Relevant methods and ideas from the machine learning community include semi-supervised learning learning from heterogeneous data and incremental learning.,"['transductive learning', 're-sequencing', 'genotyping', 'data mining', 'statistics', 'Low-level microarray analysis', 'microarray', 'low-level analysis', 'learning from heterogeneous data', 'semi-supervised learning', 'gene expression estimation', 'incremental learning', 'transcript discovery', 'heterogeneous data']",105,14
131,133,Ada95 is an object-oriented programming language. Pack-ages are basic program units in Ada 95 to support OO programming which allow the specification of groups of logically related entities. Thus the cohesion of a package is mainly about how tightly the entities are encapsulated in the package. This paper discusses the relationships among these entities based on dependence analysis and presents the properties to obtain these dependencies. Based on these the paper proposes an approach to measure the package cohesion which satisfies the properties that a good measure should have.,"['OO programming', 'Object-Oriented', 'Cohesion', 'Measurement', 'measure', 'cohesion', 'Ada95', 'dependence']",89,8
132,134,Public administrations of all over the world invest an enormous amount of resources in e-government. How the success of e-government can be measured is often not clear. E-government involves many aspects of public administration ranging from introducing new technology to business process re-)engineering. The measurement of the effectiveness of e-government is a complicated endeavor. In this paper current practices of e-government measurement are evaluated. A number of limitations of current measurement instruments are identified. Measurement focuses predominantly on the front primarily counting the number of services offered and not on the back-office processes. Interpretation of measures is difficult as all existing measurement instruments lack a framework depicting the relationships between the indicators and the use of resources. The different measures may fit the aim of the owners of the e-governmental services however due to conflicting aims and priorities little agreement exists on a uniform set of measures needed for comparison of e-government development. Traditional methods of measuring e-government impact and resource usage fall short of the richness of data required for the effective evaluation of e-government strategies.,"['benchmark', 'law', 'evaluation', 'public administration', 'E-government', 'business process', 'e-government', 'interoperability', 'measurement', 'architectures']",177,10
133,135,We give the first on-line poly-logarithmic competitve algorithm for minimizing average flow time with preemption on related machines i.e. when machines can have different speeds. This also yields the first poly-logarithmic polynomial time approximation algorithm for this problem. More specifically we give an O(log 2 P · log S)-competitive algorithm where P is the ratio of the biggest and the smallest processing time of a job and S is the ratio of the highest and the smallest speed of a machine. Our algorithm also has the nice property that it is non-migratory. The scheduling algorithm is based on the concept of making jobs wait for a long enough time before scheduling them on slow machines.,"['multiprocessor environment', 'average flow time', 'Scheduling', 'related machines', 'processing time', 'preemption', 'competitive ratio', 'non-migratory algorithm', 'flow-time', 'poly-logarithmic factor', 'scheduling', 'approximation algorithms']",115,12
134,136,In this paper we propose a new way to automatically model and predict human behavior of receiving and disseminating information by analyzing the contact and content of personal communications. A personal profile called CommunityNet is established for each individual based on a novel algorithm incorporating contact content and time information simultaneously. It can be used for personal social capital management. Clusters of CommunityNets provide a view of informal networks for organization management. Our new algorithm is developed based on the combination of dynamic algorithms in the social network field and the semantic content classification methods in the natural language processing and machine learning literatures. We tested CommunityNets on the Enron Email corpus and report experimental results including filtering prediction and recommendation capabilities. We show that the personal behavior and intention are somewhat predictable based on these models. For instance quot;to whom a person is going to send a specific email&quot can be predicted by one's personal social network and content analysis. Experimental results show the prediction accuracy of the proposed adaptive algorithm is 58 better than the social network-based predictions and is 75 better than an aggregated model based on Latent Dirichlet Allocation with social network enhancement. Two online demo systems we developed that allow interactive exploration of CommunityNet are also discussed.,"['information dissemination', 'personal information management', 'user behavior modeling']",212,3
135,137,Object-oriented software development practices are being rapidly adopted within increasingly complex systems including reactive real-time and concurrent system applications. While data modeling is performed very well under current object-oriented development practices behavioral modeling necessary to capture critical information in real-time reactive and concurrent systems is often lacking. Addressing this deficiency we offer an approach for modeling and analyzing concurrent object-oriented software designs through the use of behavioral design patterns allowing us to map stereotyped UML objects to colored Petri net CPN representations in the form of reusable templates. The resulting CPNs are then used to model and analyze behavioral properties of the software architecture applying the results of the analysis to the original software design.,"['COMET', 'Behavioral Design Patterns', 'Colored Petri Nets', 'Software Architecture']",115,4
136,138,Motivated by recent surfacing viruses that can spread over the air interfaces in this paper we investigate the potential disastrous threat of node compromise spreading in wireless sensor networks. Originating from a single infected node we assume such a compromise can propagate to other sensor nodes via communication and preestablished mutual trust. We focus on the possible epidemic breakout of such propagations where the whole network may fall victim to the attack. Based on epidemic theory we model and analyze this spreading process and identify key factors determining potential outbreaks. In particular we perform our study on random graphs precisely constructed according to the parameters of the network such as distance key sharing constrained communication and node recovery thereby reflecting the true characteristics therein. The analytical results provide deep insights in designing potential defense strategies against this threat. Furthermore  through extensive simulations we validate our model and perform investigations on the system dynamics. Index Terms Sensor Networks Epidemiology Random Key Predistribution Random Graph.,"['Epidemiology', 'Random Key Predistribution', 'Sensor Networks', 'Random Graph']",163,4
137,139,The literature is very broad considering routing protocols in wireless sensor networks WSNs). However security of these routing protocols has fallen beyond the scope so far. Routing is a fundamental functionality in wireless networks thus hostile interventions aiming to disrupt and degrade the routing service have a serious impact on the overall operation of the entire network. In order to analyze the security of routing protocols in a precise and rigorous way we propose a formal framework encompassing the definition of an adversary model as well as the general definition of secure routing in sensor networks. Both definitions take into account the feasible goals and capabilities of an adversary in sensor environments and the variety of sensor routing protocols. In spirit our formal model is based on the simulation paradigm that is a successfully used technique to prove the security of various cryptographic protocols. However we also highlight some differences between our model and other models that have been proposed for wired or wireless networks. Finally we illustrate the practical usage of our model by presenting the formal description of a simple attack against an authenticated routing protocol which is based on the well-known TinyOS routing.,"['Provable Security', 'Simulatability', 'Sensor Networks', 'Routing Protocols', 'Adversary Model']",196,5
138,140,We investigate whether it is possible to encrypt a database and then give it away in such a form that users can still access it but only in a restricted way. In contrast to conventional privacy mechanisms that aim to prevent any access to individual records we aim to restrict the set of queries that can be feasibly evaluated on the encrypted database. We start with a simple form of database obfuscation which makes database records indistinguishable from lookup functions . The only feasible operation on an obfuscated record is to look up some attribute Y by supplying the value of another attribute X that appears in the same record i.e. someone who does not know X cannot feasibly retrieve Y . We then i generalize our construction to conjunctions of equality tests on any attributes of the database and ii achieve a new property we call group privacy. This property ensures that it is easy to retrieve individual records or small subsets of records from the encrypted database by identifying them precisely but mass harvesting queries matching a large number of records are computationally infeasible. Our constructions are non-interactive. The database is transformed in such a way that all queries except those ex-plicitly allowed by the privacy policy become computationally infeasible i.e. our solutions do not rely on any access-control software or hardware.,"['Database privacy', 'Obfuscation']",224,2
139,141,Peer-to-Peer P2P  data integration systems have recently attracted significant attention for their ability to manage and share data dispersed over different peer sources. While integrating data for answering user queries it often happens that inconsistencies arise because some integrity constraints specified on peers global schemas may be violated. In these cases we may give semantics to the inconsistent system by suitably repairing the retrieved data as typically done in the context of traditional data integration systems. However  some specific features of P2P systems such as peer autonomy and peer preferences e.g. different source trusting  should be properly addressed to make the whole approach effective. In this paper we face these issues that were only marginally considered in the literature. We first present a formal framework for reasoning about autonomous peers that exploit individual preference criteria in repairing the data. The idea is that queries should be answered over the best possible database repairs with respect to the preferences of all peers i.e. the states on which they are able to find an agreement. Then we investigate the computational complexity of dealing with peer agreements and of answering queries in P2P data integration systems. It turns out that considering peer preferences makes these problems only mildly harder than in traditional data integration systems.,"['Peer-to-Peer Systems', 'Data Integration Systems']",212,2
140,142,In this paper we study market share rules rules that have a certain market share statistic associated with them. Such rules are particularly relevant for decision making from a business perspective. Motivated by market share rules in this paper we consider statistical quantitative rules SQ rules that are quantitative rules in which the RHS can be any statistic that is computed for the segment satisfying the LHS of the rule. Building on prior work we present a statistical approach for learning all significant SQ rules i.e. SQ rules for which a desired statistic lies outside a confidence interval computed for this rule. In particular we show how resampling techniques can be effectively used to learn significant rules. Since our method considers the significance of a large number of rules in parallel it is susceptible to learning a certain number of quot;false&quot rules. To address this we present a technique that can determine the number of significant SQ rules that can be expected by chance alone and suggest that this number can be used to determine a quot;false discovery rate&quot for the learning procedure. We apply our methods to online consumer purchase data and report the results.,"['Rule discovery', 'nonparametric methods', 'statistical quantitative rules', 'market share rules', 'resampling']",196,5
141,143,Prolonging the network lifetime is one of the most important designing objectives in wireless sensor networks WSN). We consider a heterogeneous cluster-based WSN which consists of two types of nodes powerful cluster-heads and basic sensor nodes. All the nodes are randomly deployed in a specific area. To better balance the energy dissipation we use a simple mixed communication modes where the sensor nodes can communicate with cluster-heads in either single-hop or multi-hop mode. Given the initial energy of the basic sensor nodes we derive the optimal communication range and identify the optimal mixed communication mode to maximize the WSN's lifetime through optimizations. Moreover we also extend our model from 2-D space to 3-D space.,"['energy optimization', 'clustering', 'optimization', 'heterogeneous cluster-based sensor network', 'optimal transmission range', 'Wireless sensor networks', 'network lifetime', 'Voronoi cell', 'numerical model']",114,9
142,144,In this paper we study how we can design an effective parallel crawler. As the size of the Web grows it becomes imperative to parallelize a crawling process in order to finish downloading pages in a reasonable amount of time. We first propose multiple architectures for a parallel crawler and identify fundamental issues related to parallel crawling. Based on this understanding we then propose metrics to evaluate a parallel crawler and compare the proposed architectures using 40 million pages collected from the Web. Our results clarify the relative merits of each architecture and provide a good guideline on when to adopt which architecture.,"['guideline', 'architecture', 'model evaluation', 'parallel crawler', 'Web Spider', 'Web Crawler', 'Parallelization']",103,7
143,145,Unlike non-time-critical applications like email and file transfer  network games demand timely data delivery to maintain the seemingly interactive presence of players in the virtual game world. Yet the inherently large transmission delay mean and variance of 3G cellular links make on-time game data delivery difficult. Further complicating the timely game data delivery problem is the frequent packet drops at these links due to inter-symbol interference fading and shadowing at the physical layer. In this paper we propose a proxy architecture that enhances the timeliness and reliability of data delivery of interactive games over 3G wireless networks. In particular a performance enhancing proxy is designed to optimize a new time-critical data type  variable-deadline data where the utility of a datum is inversely proportional to the time required to deliver it. We show how a carefully designed and configured proxy can noticeably improve the delivery of network game data.,"['Wireless Networks', 'congestion control', '3G wireless network', 'RLC configuration', 'proxy architecture', 'Network Gaming', 'time critical data', 'loss-optimized']",148,8
144,146,In this paper we present a method for real-time visual simulation of diverse dynamic phenomena using programmable graphics hardware. The simulations we implement use an extension of cellular automata known as the coupled map lattice CML). CML represents the state of a dynamic system as continuous values on a discrete lattice. In our implementation we store the lattice values in a texture and use pixel-level programming to implement simple next-state computations on lattice nodes and their neighbors. We apply these computations successively to produce interactive visual simulations of convection reaction-diffusion and boiling. We have built an interactive framework for building and experimenting with CML simulations running on graphics hardware and have integrated them into interactive 3D graphics applications.,"['Coupled Map Lattice', 'CML', 'dynamic phenomena', 'graphic hardware', 'simulation', 'Multipass Rendering', 'Visual Simulation', 'Reaction-Diffusion', 'Graphics Hardware']",118,9
145,147,A common measure of the quality or effectiveness of a virtual environment VE is the amount of presence it evokes in users. Presence is often defined as the sense of being there in a VE. There has been much debate about the best way to measure presence and presence researchers need and have sought a measure that is reliable valid sensitive and objective. We hypothesized that to the degree that a VE seems real it would evoke physiological responses similar to those evoked by the corresponding real environment and that greater presence would evoke a greater response. To examine this we conducted three experiments the results of which support the use of physiological reaction as a reliable valid sensitive and objective presence measure. The experiments compared participants physiological reactions to a non-threatening virtual room and their reactions to a stressful virtual height situation. We found that change in heart rate satisfied our requirements for a measure of presence change in skin conductance did to a lesser extent and that change in skin temperature did not. Moreover the results showed that inclusion of a passive haptic element in the VE significantly increased presence and that for presence evoked 30FPS gt 20FPS gt 15FPS.,"['virtual environment', 'Physiology', 'presence', 'Haptics', 'measurement', 'Presence', 'Frame Rate']",202,7
146,148,Automated trust negotiation is an approach which establishes trust between strangers through the bilateral iterative disclosure of digital credentials. Sensitive credentials are protected by access control policies which may also be communicated to the other party. Ideally sensitive information should not be known by others unless its access control policy has been satisfied. However due to bilateral information exchange information may flow to others in a variety of forms many of which cannot be protected by access control policies alone. In particular sensitive information may be inferred by observing negotiation participants behavior even when access control policies are strictly enforced. In this paper we propose a general framework for the safety of trust negotiation systems. Compared to the existing safety model our framework focuses on the actual information gain during trust negotiation instead of the exchanged messages. Thus it directly reflects the essence of safety in sensitive information protection. Based on the proposed framework we develop policy databases as a mechanism to help prevent unauthorized information inferences during trust negotiation. We show that policy databases achieve the same protection of sensitive information as existing solutions without imposing additional complications to the interaction between negotiation participants or restricting users autonomy in defining their own policies.,"['Privacy', 'Attribute-based Access Control', 'Trust Negotiation']",204,3
147,149,"We propose a new unsupervised learning technique for extracting information from large text collections. We model documents as if they were generated by a two-stage stochastic process. Each author is represented by a probability distribution over topics and each topic is represented as a probability distribution over words for that topic. The words in a multi-author paper are assumed to be the result of a mixture of each authors topic mixture. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm . We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library and learn a model with 300 topics. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models ranking of authors by topic and topics by author significant trends in the computer science literature between 1990 and 2002 parsing of abstracts by topics and authors and detection of unusual papers by specific authors. An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer.","['Gibbs sampling', 'text modeling', 'unsupervised learning']",195,3
148,150,Speed accuracy and subjective satisfaction are the most common measures for evaluating the usability of search user interfaces. However these measures do not facilitate comparisons optimally and they leave some important aspects of search user interfaces uncovered. We propose new proportional measures to supplement the current ones. Search speed is a normalized measure for the speed of a search user interface expressed in answers per minute. Qualified search speed reveals the trade-off between speed and accuracy while immediate search accuracy addresses the need to measure success in typical web search behavior where only the first few results are interesting. The proposed measures are evaluated by applying them to raw data from two studies and comparing them to earlier measures. The evaluations indicate that they have desirable features.,"['accuracy', 'speed', 'usability evaluation', 'usability measure', 'Search user interface']",127,5
149,151,Valuable 3D graphical models such as high-resolution digital scans of cultural heritage objects may require protection to prevent piracy or misuse while still allowing for interactive display and manipulation by a widespread audience. We have investigated techniques for protecting 3D graphics content and we have developed a remote rendering system suitable for sharing archives of 3D models while protecting the 3D geometry from unauthorized extraction . The system consists of a 3D viewer client that includes low-resolution versions of the 3D models and a rendering server that renders and returns images of high-resolution models according to client requests. The server implements a number of defenses to guard against 3D reconstruction attacks such as monitoring and limiting request streams and slightly perturbing and distorting the rendered images. We consider several possible types of reconstruction attacks on such a rendering server and we examine how these attacks can be defended against without excessively compromising the interactive experience for non-malicious users.,"['security', '3D models', 'digital rights management', 'remote rendering']",158,4
150,152,In order to enable the widespread use of robots in home and office environments systems with natural interaction capabilities have to be developed. A prerequisite for natural interaction is the robot's ability to automatically recognize when and how long a person's attention is directed towards it for communication. As in open environments several persons can be present simultaneously the detection of the communication partner is of particular importance. In this paper we present an attention system for a mobile robot which enables the robot to shift its attention to the person of interest and to maintain attention during interaction. Our approach is based on a method for multi-modal person tracking which uses a pan-tilt camera for face recognition two microphones for sound source localization and a laser range finder for leg detection. Shifting of attention is realized by turning the camera into the direction of the person which is currently speaking. From the orientation of the head it is decided whether the speaker addresses the robot. The performance of the proposed approach is demonstrated with an evaluation. In addition qualitative results from the performance of the robot at the exhibition part of the ICVS'03 are provided.,"['Attention', 'Multi-modal person tracking', 'Human-robot-interaction']",196,3
151,153,In this paper we describe a framework for a personalization system to systematically induce desired emotion and attention related states and promote information processing in viewers of online advertising and e-commerce product information. Psychological Customization entails personalization of the way of presenting information user interface visual layouts modalities structures per user to create desired transient psychological effects and states such as emotion attention involvement presence persuasion and learning. Conceptual foundations and empiric evidence for the approach are presented.,"['E-commerce', 'personalization emotion', 'advertising', 'persuasion']",78,4
152,154,Today watermarking techniques have been extended from the multimedia context to relational databases so as to protect the ownership of data even after the data are published or distributed. However  all existing watermarking schemes for relational databases are secret key based  thus require a secret key to be presented in proof of ownership. This means that the ownership can only be proven once to the public e.g. to the court). After that the secret key is known to the public and the embedded watermark can be easily destroyed by malicious users. Moreover most of the existing techniques introduce distortions to the underlying data in the watermarking process either by modifying least significant bits or exchanging categorical values. The distortions inevitably reduce the value of the data. In this paper we propose a watermarking scheme by which the ownership of data can be publicly proven by anyone as many times as necessary. The proposed scheme is distortion-free  thus suitable for watermarking any type of data without fear of error constraints. The proposed scheme is robust against typical database attacks including tuple/attribute insertion/deletion ran-dom/selective value modification data frame-up and additive attacks,"['certificate', 'ownership protection', 'public verifiability', 'Relational database', 'watermark']",189,5
153,155,A person working with diverse information sources--with possibly different formats and information models--may recognize and wish to express conceptual structures that are not explicitly present in those sources. Rather than replicate the portions of interest and recast them into a single combined data source we leave base information where it is and superimpose a conceptual model that is appropriate to the task at hand. This superimposed model can be distinct from the model(s employed by the sources in the base layer. An application that superimposes a new conceptual model over diverse sources with varying capabilities needs to accommodate the various types of information and differing access protocols for the base information sources. The Superimposed Pluggable Architecture for Contexts and Excerpts SPARCE defines a collection of architectural abstractions placed between superimposed and base applications to demarcate and revisit information elements inside base sources and provide access to content and context for elements inside these sources. SPARCE accommodates new base information types without altering existing superimposed applications. In this paper we briefly introduce several superimposed applications that we have built and describe the conceptual model each superimposes. We then focus on the use of context in superimposed applications. We describe how SPARCE supports context and excerpts. We demonstrate how SPARCE facilitates building superimposed applications by describing its use in building our two quite diverse applications.,"['software architecture', 'context', 'superimposed information', 'excerpts', 'Conceptual modelling', 'SPARCE .']",223,6
154,156,Along with the development of multimedia and wireless networking technologies mobile multimedia applications are playing more important roles in information access. Quality of Service QoS is a critical issue in providing guaranteed service in a low bandwidth wireless environment. To provide Bluetooth-IP services with differentiated quality requirements a QoS-centric cascading mechanism is proposed in this paper. This innovative mechanism composed of intra-piconet resource allocation inter-piconet handoff and Bluetooth-IP access modules is based on the Bluetooth Network Encapsulation Protocol BNEP operation scenario. From our simulations the handoff connection time for a Bluetooth device is up to 11.84 s and the maximum average transmission delay is up to 4e-05 s when seven devices join a piconet simultaneously. Increasing the queue length for the Bluetooth-IP access system will decrease the traffic loss rate by 0.02 per 1000 IP packets at the expense of a small delay performance.,"['handoff', 'resource allocation', 'quality of service', 'Bluetooth-IP access system', 'BNEP protocol']",144,5
155,157,This paper describes a question answering system that is designed to capitalize on the tremendous amount of data that is now available online. Most question answering systems use a wide variety of linguistic resources. We focus instead on the redundancy available in large corpora as an important resource. We use this redundancy to simplify the query rewrites that we need to use and to support answer mining from returned snippets. Our system performs quite well given the simplicity of the techniques being utilized. Experimental results show that question answering accuracy can be greatly improved by analyzing more and more matching passages. Simple passage ranking and n-gram extraction techniques work well in our system making it efficient to use with many backend retrieval engines.,"['information retrieval', 'TREC QA', 'n-gram extraction techniques', 'question answering system', 'redundancy in large corpora', 'facilitates answer mining', 'automatic QA', 'information extraction', 'machine learning', 'natural language processing', 'rewrite query', 'simple passage ranking']",123,12
156,158,To deal with the problem of too many results returned from an E-commerce Web database in response to a user query this paper proposes a novel approach to rank the query results. Based on the user query we speculate how much the user cares about each attribute and assign a corresponding weight to it. Then for each tuple in the query result each attribute value is assigned a score according to its desirableness to the user. These attribute value scores are combined according to the attribute weights to get a final ranking score for each tuple. Tuples with the top ranking scores are presented to the user first. Our ranking method is domain independent and requires no user feedback. Experimental results demonstrate that this ranking method can effectively capture a user's preferences.,"['QRRE', 'human factors', 'rank the query results', 'Query result ranking', 'many query result problem', 'query result ranking', 'PIR', 'E-commerce', 'attribute preference', 'experimentation', 'Attribute weight assignment', 'algorithms', 'attribute value', 'e-commerce web databases', 'design']",132,15
157,159,The heterogeneous Web exacerbates IR problems and short user queries make them worse. The contents of web documents are not enough to find good answer documents. Link information and URL information compensates for the insufficiencies of content information. However static combination of multiple evidences may lower the retrieval performance . We need different strategies to find target documents according to a query type. We can classify user queries as three categories the topic relevance task the homepage finding task and the service finding task. In this paper a user query classification scheme is proposed. This scheme uses the difference of distribution mutual information  the usage rate as anchor texts and the POS information for the classification. After we classified a user query we apply different algorithms and information for the better results. For the topic relevance task we emphasize the content information on the other hand for the homepage finding task we emphasize the Link information and the URL information. We could get the best performance when our proposed classification method with the OKAPI scoring algorithm was used.,"['web document', 'model', 'IR', 'Link Information', 'URL Information', 'query', 'Query Classification', 'task', 'information', 'improvement', 'Combination of Multiple Evidences', 'rate', 'frequency', 'URL']",178,14
158,160,In our research on superimposed information management we have developed applications where information elements in the superimposed layer serve to annotate comment restructure and combine selections from one or more existing documents in the base layer. Base documents tend to be unstructured or semi-structured HTML pages Excel spreadsheets and so on with marks delimiting selections. Selections in the base layer can be programmatically accessed via marks to retrieve content and context. The applications we have built to date allow creation of new marks and new superimposed elements that use marks but they have been browse-oriented and tend to expose the line between superimposed and base layers. Here we present a new access capability called bi-level queries that allows an application or user to query over both layers as a whole. Bi-level queries provide an alternative style of data integration where only relevant portions of a base document are mediated not the whole document and the superimposed layer can add information not present in the base layer. We discuss our framework for superimposed information management an initial implementation of a bi-level query system with an XML Query interface and suggest mechanisms to improve scalability and performance.,"['SPARCE', 'system', 'document', 'Superimposed information management', 'hyperlink', 'management', 'Information integration', 'query', 'Bi-level queries', 'METAXPath', 'implementation', 'information', 'superimposed', 'RIDPAD']",195,14
159,161,Most of the theoretical work on sampling has addressed the inversion of general traffic properties such as flow size distribution  average flow size or total number of flows. In this paper we make a step towards understanding the impact of packet sampling on individual flow properties. We study how to detect and rank the largest flows on a link. To this end we develop an analytical model that we validate on real traces from two networks. First we study a blind ranking method where only the number of sampled packets from each flow is known. Then we propose a new method protocol-aware ranking where we make use of the packet sequence number when available in transport header to infer the number of non-sampled packets from a flow and hence to improve the ranking. Surprisingly our analytical and experimental results indicate that a high sampling rate 10 and even more depending on the number of top flows to be ranked is required for a correct blind ranking of the largest flows. The sampling rate can be reduced by an order of magnitude if one just aims at detecting these flows or by using the protocol-aware method.,"['performance evaluation', 'Packet sampling', 'validation with real traces', 'largest flow detection and ranking']",195,4
160,162,Web navigation plays an important role in exploring public interconnected data sources such as life science data. A navigational query in the life science graph produces a result graph which is a layered directed acyclic graph DAG). Traversing the result paths in this graph reaches a target object set TOS). The challenge for ranking the target objects is to provide recommendations that reflect the relative importance of the retrieved object as well as its relevance to the specific query posed by the scientist. We present a metric layered graph PageRank lgPR to rank target objects based on the link structure of the result graph. LgPR is a modification of PageRank it avoids random jumps to respect the path structure of the result graph. We also outline a metric layered graph ObjectRank lgOR which extends the metric ObjectRank to layered graphs. We then present an initial evaluation of lgPR. We perform experiments on a real-world graph of life sciences objects from NCBI and report on the ranking distribution produced by lgPR. We compare lgPR with PageRank. In order to understand the characteristics of lgPR an expert compared the Top K target objects publications in the PubMed source produced by lgPR and a word-based ranking method that uses text features extracted from an external source such as Entrez Gene to rank publications.,"['Navigational Query', 'PageRank', 'Ranking', 'Link Analysis']",220,4
161,163,Vertical search is a promising direction as it leverages domain-specific knowledge and can provide more precise information for users. In this paper we study the Web object-ranking problem one of the key issues in building a vertical search engine. More specifically we focus on this problem in cases when objects lack relationships between different Web communities  and take high-quality photo search as the test bed for this investigation. We proposed two score fusion methods that can automatically integrate as many Web communities Web forums with rating information as possible. The proposed fusion methods leverage the hidden links discovered by a duplicate photo detection algorithm and aims at minimizing score differences of duplicate photos in different forums . Both intermediate results and user studies show the proposed fusion methods are practical and efficient solutions to Web object ranking in cases we have described. Though the experiments were conducted on high-quality photo ranking  the proposed algorithms are also applicable to other ranking problems such as movie ranking and music ranking,"['image search', 'Web objects', 'ranking']",168,3
162,164,While users disseminate various information in the open and widely distributed environment of the Semantic Web determination of who shares access to particular information is at the center of looming privacy concerns. We propose a real-world oriented information sharing system that uses social networks. The system automatically obtains users social relationships by mining various external sources. It also enables users to analyze their social networks to provide awareness of the information dissemination process. Users can determine who has access to particular information based on the social relationships and network analysis.,"['Information sharing', 'Social network']",90,2
163,165,Enterprises in the public and private sectors have been making their large spatial data archives available over the Internet . However interactive work with such large volumes of online spatial data is a challenging task. We propose two efficient approaches to remote access to large spatial data. First we introduce a client-server architecture where the work is distributed between the server and the individual clients for spatial query evaluation data visualization and data management. We enable the minimization of the requirements for system resources on the client side while maximizing system responsiveness as well as the number of connections one server can handle concurrently. Second for prolonged periods of access to large online data we introduce APPOINT an Approach for Peer-to-Peer Offloading the INTernet). This is a centralized peer-to-peer approach that helps Internet users transfer large volumes of online data efficiently. In APPOINT active clients of the client-server architecture act on the server's behalf and communicate with each other to decrease network latency improve service bandwidth and resolve server congestions.,"['GIS', 'Internet', 'Client/server', 'Peer-to-peer']",170,4
164,166,An increasing amount of heterogeneous information about scientific research is becoming available on-line. This potentially allows users to explore the information from multiple perspectives and derive insights and not just raw data about a topic of interest. However most current scientific information search systems lag behind this trend being text-based they are fundamentally incapable of dealing with multimedia data. An even more important limitation is that their information environments are information-centric and therefore are not suitable if insights are desired. Towards this goal in this paper we describe the design of a system called ResearchExplorer which facilitates exploring multimedia scientific data to gain insights. This is accomplished by providing an interaction environment for insights where users can explore multimedia scientific information sources. The multimedia information is united around the notion of research event and can be accessed in a unified way. Experiments are conducted to show how ResearchExplorer works and how it cardinally differs from other search systems.,"['Event', 'Interaction Environment', 'Spatio-Temporal Data', 'Multimedia Data', 'Research Event', 'Insight', 'Exploration']",158,7
165,167,Cognitive information complexity measure is based on cognitive informatics which helps in comprehending the software characteristics. For any complexity measure to be robust Weyuker properties must be satisfied to qualify as good and comprehensive one. In this paper an attempt has also been made to evaluate cognitive information complexity measure in terms of nine Weyuker properties through examples. It has been found that all the nine properties have been satisfied by cognitive information complexity measure and hence establishes cognitive information complexity measure based on information contained in the software as a robust and well-structured one.,"['Weighted information count', 'cognitive weight', 'basic control structures', 'cognitive information complexity measure', 'cognitive information complexity unit']",95,5
166,168,From experience with wireless sensor networks it has become apparent that dynamic reprogramming of the sensor nodes is a useful feature. The resource constraints in terms of energy memory and processing power make sensor network reprogramming a challenging task. Many different mechanisms for reprogramming sensor nodes have been developed ranging from full image replacement to virtual machines. We have implemented an in-situ run-time dynamic linker and loader that use the standard ELF object file format. We show that run-time dynamic linking is an effective method for reprogramming even resource constrained wireless sensor nodes. To evaluate our dynamic linking mechanism we have implemented an application-specific virtual machine and a Java virtual machine and compare the energy cost of the different linking and execution models. We measure the energy consumption and execution time overhead on real hardware to quantify the energy costs for dynamic linking. Our results suggest that while in general the overhead of a virtual machine is high a combination of native code and virtual machine code provide good energy efficiency. Dynamic run-time linking can be used to update the native code even in heterogeneous networks.,"['Operating systems', 'Virtual machines', 'Dynamic linking', 'Wireless sensor networks', 'Embedded systems']",186,5
167,169,S e nsor net wor k c omput i ng can be char act eri zed as resource-const r ai ned distributed computing using unreliable low bandwidth communication . This combination of characteristics poses significant software development and maintenance challenges. Effective and efficient debugging tools for sensor network are thus critical. Existent development tools such as TOSSIM EmStar ATEMU and Avrora provide useful debugging support but not with the fidelity scale and functionality that we believe are sufficient to meet the needs of the next generation of applications. In this paper we propose a debugger called S 2 DB based on a distributed full system sensor network simulator with high fidelity and scalable performance DiSenS. By exploiting the potential of DiSenS as a scalable full system simulator S 2 DB extends conventional debugging methods by adding novel device level program source level group level and network level debugging abstractions. The performance evaluation shows that all these debugging features introduce overhead that is generally less than 10 into the simulator and thus making S 2 DB an efficient and effective debugging tool for sensor networks.,"['Simulation', 'Debugging', 'Sensor Network']",184,3
168,170,Computing and maintaining network structures for efficient data aggregation incurs high overhead for dynamic events where the set of nodes sensing an event changes with time. Moreover structured approaches are sensitive to the waiting-time which is used by nodes to wait for packets from their children before forwarding the packet to the sink. Although structure-less approaches can address these issues the performance does not scale well with the network size. We propose a semi-structured approach that uses a structure-less technique locally followed by Dynamic Forwarding on an implicitly constructed packet forwarding structure to support network scalability. The structure ToD is composed of multiple shortest path trees. After performing local aggregation  nodes dynamically decide the forwarding tree based on the location of the sources. The key principle behind ToD is that adjacent nodes in a graph will have low stretch in one of these trees in ToD thus resulting in early aggregation of packets. Based on simulations on a 2000 nodes network and real experiments on a 105 nodes Mica2-based network we conclude that efficient aggregation in large scale networks can be achieved by our semi-structured approach.,"['Anycasting', 'ToD', 'Structure-free', 'Data Aggregation']",186,4
169,171,Mining frequent structural patterns from graph databases is an interesting problem with broad applications. Most of the previous studies focus on pruning unfruitful search subspaces effectively but few of them address the mining on large disk-based databases. As many graph databases in applications cannot be held into main memory scalable mining of large disk-based graph databases remains a challenging problem. In this paper we develop an effective index structure ADI for adjacency index to support mining various graph patterns over large databases that cannot be held into main memory. The index is simple and efficient to build. Moreover the new index structure can be easily adopted in various existing graph pattern mining algorithms. As an example  we adapt the well-known gSpan algorithm by using the ADI structure. The experimental results show that the new index structure enables the scalable graph pattern mining over large databases. In one set of the experiments the new disk-based method can mine graph databases with one million graphs while the original gSpan algorithm can only handle databases of up to 300 thousand graphs. Moreover our new method is faster than gSpan when both can run in main memory.,"['Graph mining', 'ADI Index structure', 'graph database', 'GRaph databases', 'index', 'Frequent graph pattern mining', 'DFS code', 'Gspan algorithm', 'Memory based databases', 'Edge table', 'Disk bases databases', 'Subgraph mine', 'Adjacency list representation', 'frequent graph pattern']",193,14
170,172,The IP Multimedia Subsystem IMS defined by Third Generation Partnership Projects 3GPP and 3GPP2 is a technology designed to provide robust multimedia services across roaming boundaries and over diverse access technologies with promising features like quality-of-service QoS reliability and security. The IMS defines an overlay service architecture that merges the paradigms and technologies of the Internet with the cellular and fixed telecommunication worlds. Its architecture enables the efficient provision of an open set of potentially highly integrated multimedia services combining web browsing email instant messaging presence VoIP video conferencing application sharing telephony unified messaging multimedia content delivery etc. on top of possibly different network technologies. As such IMS enables various business models for providing seamless business and consumer multimedia applications. In this communication converged world the challenging issues are security quality of service QoS and management  administration. In this paper our focus is to manage secure access to multimedia services and applications based on SIP and HTTP on top of IP Multimedia Subsystem IMS). These services include presence video conferencing messaging video broadcasting and push to talk etc. We will utilize Generic Bootstrapping Architecture GBA model to authenticate multimedia applications before accessing these multimedia services offered by IMS operators. We will make enhancement in GBA model to access these services securely by introducing Authentication Proxy AP which is responsible to implement Transport Layer Security TLS for HTTP and SIP communication. This research work is part of Secure Service Provisioning SSP Framework for IP Multimedia System at Fokus Fraunhofer IMS 3Gb Testbed.,"['GLMS/XDMS', 'Generic Authentication Architecture', 'Transport Layer Security', 'IP Multimedia System', 'Diameter proxy', 'Authentication Proxy', 'GBA', 'TLS', 'Transport layer security', 'General bootstrapping architecture', 'AP', 'Authentication proxy', 'Fokus IMS Testbed', 'Security and Privacy', 'TLS Tunnel end points', 'Signalling protocols', 'Generic Bootstrapping Architecture', 'IP multimedia subsystem', 'IMS platform', 'NAF', 'Network authentication function']",251,21
171,173,In-network aggregation is an essential primitive for performing queries on sensor network data. However most aggregation algorithms assume that all intermediate nodes are trusted. In contrast the standard threat model in sensor network security assumes that an attacker may control a fraction of the nodes which may misbehave in an arbitrary Byzantine manner. We present the first algorithm for provably secure hierarchical in-network data aggregation. Our algorithm is guaranteed to detect any manipulation of the aggregate by the adversary beyond what is achievable through direct injection of data values at compromised nodes. In other words the adversary can never gain any advantage from misrepresenting intermediate aggregation computations. Our algorithm incurs only O log 2 n  node congestion supports arbitrary tree-based aggregator topologies and retains its resistance against aggregation manipulation in the presence of arbitrary numbers of malicious nodes. The main algorithm is based on performing the SUM aggregation securely by first forcing the adversary to commit to its choice of intermediate aggregation results and then having the sensor nodes independently verify that their contributions to the aggregate are correctly incorporated. We show how to reduce secure MEDIAN  COUNT  and AVERAGE to this primitive.,"['secure hierarchical data aggregation protocol', 'commitment tree', 'result checking', 'sensor network', 'Sensor Networks', 'query dissemination', 'in-network data aggregation', 'aggregation commit', 'congestion complexity', 'Secure aggregation', 'algorithm', 'Data aggregation', 'commitment forest']",193,13
172,174,The use of middleware eases the development of distributed applications by abstracting the intricacies communication and coordination among software components of the distributed network environment. In wireless sensor networks this is even trickier because of their specific issues such as addressing mobility number of sensors and energy-limited nodes. This paper describes SensorBus a message-oriented middleware MOM model for wireless sensor networks based on the publish-subscribe paradigm and that allows the free exchange of the communication mechanism among sensor nodes allowing as result the capability of using more than one communication mechanism to address the requirements of larger number of applications. We intend to provide a platform which addresses the main characteristics of wireless sensor networks and also allows the development of energy-efficient applications. SensorBus incorporates constraint and query languages which will aid the development of interactive applications. It intends with the utilization of filters reduces data movement minimizing the energy consumption of nodes.,"['constraint and query languages', 'application service', 'wireless sensor network', 'context service', 'Middleware', 'message service', 'design pattern', 'message-oriented middleware model', 'wireless sensor networks', 'application filters', 'environmental monitoring applications', 'publish-subscribe paradigm']",153,12
173,175,We investigate the design space of sensor network broadcast authentication . We show that prior approaches can be organized based on a taxonomy of seven fundamental proprieties such that each approach can satisfy at most six of the seven proprieties. An empirical study of the design space reveals possibilities of new approaches which we present in the following two new authentication protocols  RPT and LEA. Based on this taxonomy we offer guidance in selecting the most appropriate protocol based on an application's desired proprieties. Finally we pose the open challenge for the research community to devise a protocol simultaneously providing all seven properties.,"['Broadcast Authentication', 'Sensor Network', 'Taxonomy']",103,3
174,176,Many methods for classification and gene selection with microarray data have been developed. These methods usually give a ranking of genes. Evaluating the statistical significance of the gene ranking is important for understanding the results and for further biological investigations but this question has not been well addressed for machine learning methods in existing works. Here we address this problem by formulating it in the framework of hypothesis testing and propose a solution based on resampling. The proposed r-test methods convert gene ranking results into position p-values to evaluate the significance of genes. The methods are tested on three real microarray data sets and three simulation data sets with support vector machines as the method of classification and gene selection. The obtained position p-values help to determine the number of genes to be selected and enable scientists to analyze selection results by sophisticated multivariate methods under the same statistical inference paradigm as for simple hypothesis testing methods.,"['classification', 'gene selection', 'Significance of gene ranking', 'microarray data analysis']",157,4
175,177,The contour tree an abstraction of a scalar field that encodes the nesting relationships of isosurfaces can be used to accelerate isosurface extraction to identify important isovalues for volume-rendering transfer functions and to guide exploratory visualization through a flexible isosurface interface. Many real-world data sets produce unmanageably large contour trees which require meaningful simplification. We define local geometric measures for individual contours such as surface area and contained volume and provide an algorithm to compute these measures in a contour tree. We then use these geometric measures to simplify the contour trees suppressing minor topological features of the data. We combine this with a flexible isosurface interface to allow users to explore individual contours of a dataset interactively.,"['contour trees', 'topological simplification', 'Isosurfaces']",118,3
176,178,The way current search engines work leaves a large amount of information available in the World Wide Web outside their catalogues. This is due to the fact that crawlers work by following hyperlinks and a few other references and ignore HTML forms. In this paper we propose a search engine prototype that can retrieve information behind HTML forms by automatically generating queries for them. We describe the architecture some implementation details and an experiment that proves that the information is not in fact indexed by current search engines.,"['Search Engine', 'information retrieval', 'Label Extraction', 'architecture', 'Hidden Web', 'hidden web content', 'web crawler', 'SmartCrawl', 'search engine', 'experimentation', 'implementation', 'html form', 'extraction algorithm']",88,13
177,179,Braille and audio feedback based systems have vastly improved the lives of the visually impaired across a wide majority of the globe. However more than 13 million visually impaired people in the Indian sub-continent could not benefit much from such systems. This was primarily due to the difference in the technology required for Indian languages compared to those corresponding to other popular languages of the world. In this paper we describe the Sparsha toolset. The contribution made by this research has enabled the visually impaired to read and write in Indian vernaculars with the help of a computer.,"['Indian languages', 'Visual impairment', 'Braille', 'audio feedback']",98,4
178,180,This paper describes StyleCam an approach for authoring 3D viewing experiences that incorporate stylistic elements that are not available in typical 3D viewers. A key aspect of StyleCam is that it allows the author to significantly tailor what the user sees and when they see it. The resulting viewing experience can approach the visual richness and pacing of highly authored visual content such as television commercials or feature films. At the same time StyleCam allows for a satisfying level of interactivity while avoiding the problems inherent in using unconstrained camera models. The main components of StyleCam are camera surfaces which spatially constrain the viewing camera animation clips that allow for visually appealing transitions between different camera surfaces and a simple unified interaction technique that permits the user to seamlessly and continuously move between spatial-control of the camera and temporal-control of the animated transitions. Further the user's focus of attention is always kept on the content and not on extraneous interface widgets. In addition to describing the conceptual model of StyleCam its current implementation and an example authored experience we also present the results of an evaluation involving real users.,"['3D viewers', 'interaction techniques', 'camera controls', '3D navigation', '3D visualization']",189,5
179,181,Tactile displays are now becoming available in a form that can be easily used in a user interface. This paper describes a new form of tactile output. Tactons or tactile icons are structured abstract messages that can be used to communicate messages non-visually. A range of different parameters can be used for Tacton construction including  frequency amplitude and duration of a tactile pulse plus other parameters such as rhythm and location. Tactons have the potential to improve interaction in a range of different areas particularly where the visual display is overloaded limited in size or not available such as interfaces for blind people or in mobile and wearable devices . . This paper describes Tactons the parameters used to construct them and some possible ways to design them. Examples of where Tactons might prove useful in user interfaces are given.,"['multimodal interaction', 'Tactons', 'non-visual cues', 'tactile displays']",140,4
180,182,Wireless link losses result in poor TCP throughput since losses are perceived as congestion by TCP resulting in source throttling. In order to mitigate this effect 3G wireless link designers have augmented their system with extensive local retransmission mechanisms. In addition in order to increase throughput intelligent channel state based scheduling have also been introduced. While these mechanisms have reduced the impact of losses on TCP throughput and improved the channel utilization these gains have come at the expense of increased delay and rate variability. In this paper we comprehensively evaluate the impact of variable rate and variable delay on long-lived TCP performance. We propose a model to explain and predict TCP's throughput over a link with variable rate and/or delay. We also propose a network-based solution called Ack Regulator that mitigates the effect of variable rate and/or delay without significantly increasing the round trip time while improving TCP performance by up to 40%.,"['TCP', 'simulation result', '3G wireless links', 'architecture', '3G Wireless', 'congestion solution', 'performance evaluation', 'algorithm', 'Network', 'prediction model', 'wireless communication', 'Link and Rate Variation', 'design']",154,13
181,183,Students in a sophomore-level database fundamentals course were taught SQL and database concepts using both Oracle and SQL Server. Previous offerings of the class had used one or the other database. Classroom experiences suggest that students were able to handle learning SQL in the dual environment and in fact benefited from this approach by better understanding ANSI-standard versus database-specific SQL and implementation differences in the two database systems.,"['database', 'educational fundamentals', 'SQL', 'student feedbacks', 'database systems', 'course design', 'ANSI-Standard SQL', 'Oracle', 'dual environment', 'training vs. education', 'SQL Server', 'SQL Language', 'practical results', 'teaching in IT']",68,14
182,185,We present a model based on the maximum entropy method for analyzing various measures of retrieval performance such as average precision R-precision and precision-at-cutoffs. Our methodology treats the value of such a measure as a constraint on the distribution of relevant documents in an unknown list and the maximum entropy distribution can be determined subject to these constraints. For good measures of overall performance such as average precision the resulting maximum entropy distributions are highly correlated with actual distributions of relevant documents in lists as demonstrated through TREC data for poor measures of overall performance the correlation is weaker. As such the maximum entropy method can be used to quantify the overall quality of a retrieval measure. Furthermore for good measures of overall performance such as average precision we show that the corresponding maximum entropy distributions can be used to accurately infer precision-recall curves and the values of other measures of performance and we demonstrate that the quality of these inferences far exceeds that predicted by simple retrieval measure correlation as demonstrated through TREC data.,"['Average Precision', 'Maximum Entropy', 'Evaluation']",175,3
183,186,The slowing pace of commodity microprocessor performance improvements combined with ever-increasing chip power demands has become of utmost concern to computational scientists . As a result the high performance computing community is examining alternative architectures that address the limitations of modern cache-based designs. In this work we examine the potential of using the forthcoming STI Cell processor as a building block for future high-end computing systems. Our work contains several novel contributions. First we introduce a performance model for Cell and apply it to several key scientific computing kernels dense matrix multiply sparse matrix vector multiply stencil computations  and 1D/2D FFTs. The difficulty of programming Cell which requires assembly level intrinsics for the best performance makes this model useful as an initial step in algorithm design and evaluation. Next we validate the accuracy of our model by comparing results against published hardware results as well as our own implementations on the Cell full system simulator. Additionally we compare Cell performance to benchmarks run on leading superscalar AMD Opteron VLIW Intel Itanium2 and vector Cray X1E architectures. Our work also explores several different mappings of the kernels and demonstrates a simple and effective programming model for Cell's unique architecture . Finally we propose modest microarchitectural modifications that could significantly increase the efficiency of double-precision calculations. Overall results demonstrate the tremendous potential of the Cell architecture for scientific computations in terms of both raw performance and power efficiency.,"['FFT', 'sparse matrix', 'GEMM', 'Stencil', 'SpMV', 'three level memory', 'Cell processor']",236,7
184,188,Component Based Development aims at constructing software through the inter-relationship between pre-existing components. However these components should be bound to a specific application domain in order to be effectively reused. Reusable domain components and their related documentation are usually stored in a great variety of data sources. Thus a possible solution for accessing this information is to use a software layer that integrates different component information sources. We present a component information integration data layer based on mediators. Through mediators domain ontology acts as a technique/formalism for specifying ontological commitments or agreements between component users and providers enabling more accurate software component information search.,"['Domain Engineering', 'Software Classification and Identification', 'Component Based Engineering', 'Component Repositories']",104,4
185,189,We present enhancements for UDDI  DAML-S registries allowing cooperative discovery and selection of Web services with a focus on personalization. To find the most useful service in each instance of a request not only explicit parameters of the request have to be matched against the service offers. Also user preferences or implicit assumptions of a user with respect to common knowledge in a certain domain have to be considered to improve the quality of service provisioning. In the area of Web services the notion of service ontologies together with cooperative answering techniques can take a lot of this responsibility. However without quality assessments for the relaxation of service requests and queries a personalized service discovery and selection is virtually impossible. This paper focuses on assessing the semantic meaning of query relaxation plans over multiple conceptual views of the service ontology each one representing a soft query constraint of the user request. Our focus is on the question what constitutes a minimum amount of necessary relaxation to answer each individual request in a cooperative manner. Incorporating such assessments as early as possible we propose to integrate ontology-based discovery directly into UDDI directories or query facilities in service provisioning portals. Using the quality assessments presented here this integration promises to propel today's Web services towards an intuitive user-centered service provisioning.,"['discovery of possible services', 'Tree-shaped clipping of ontologies', 'selection of the most useful', 'personalization', 'subsequent execution', 'The generalization throughout ontologies', 'Relaxing multiple ontologies', 'user profiling', 'Universal Description Discovery and Integration', 'Web services', 'Semantic Web', 'Web Service Definition Language', 'preference-based service provisioning', 'Domain-specific understanding of concepts', 'generalization hierarchy of concepts', 'cooperative service discovery', 'ontology resembles common knowledge']",218,17
186,190,Word prediction can be used for enhancing the communication ability of persons with speech and language impair-ments . In this work we explore two methods of adapting a language model to the topic of conversation and apply these methods to the prediction of fringe words.,"['topic modeling', 'accuracy of prediction method', 'language modeling', 'prediction of fringe words', 'increase probability of related words', 'conversations in the Switchboard corpus', 'communication rate', 'core vocabulary', 'immediate prediction', 'identify current topic of conversation', 'AAC', 'construct a new language model', 'Word prediction', 'decrease probability of unrelated words', 'fringe vocabulary', 'prediction window size']",45,16
187,191,In this paper we introduce a probabilistic framework to exploit hierarchy structure sharing and duration information for topic transition detection in videos. Our probabilistic detection framework is a combination of a shot classification step and a detection phase using hierarchical probabilistic models. We consider two models in this paper the extended Hierarchical Hidden Markov Model HHMM and the Coxian Switching Hidden semi-Markov Model S-HSMM because they allow the natural decomposition of semantics in videos including shared structures to be modeled directly and thus enabling efficient inference and reducing the sample complexity in learning. Additionally the S-HSMM allows the duration information to be incorporated consequently the modeling of long-term dependencies in videos is enriched through both hierarchical and duration modeling . Furthermore the use of the Coxian distribution in the S-HSMM makes it tractable to deal with long sequences in video. Our experimentation of the proposed framework on twelve educational and training videos shows that both models outperform the baseline cases flat HMM and HSMM and performances reported in earlier work in topic detection . The superior performance of the S-HSMM over the HHMM verifies our belief that duration information is an important factor in video content modeling.,"['Coxian', 'hierarchical hidden markov model', 'modeling temporal correlation', 'extended Hierarchical Hidden Markov Model', 'Educational Videos', 'shot-based semantic classification', 'domain knowledge', 'Coxian Switching Hidden semi-Markov Model', 'A variety in directional styles', 'Topic Transition Detection', 'typical durations of important semantics', 'their semantically shared substructures', 'model educational video content', 'semantic relationship of neighborhood scenes', 'Hierarchical Markov (Semi-Markov) Models', 'topic transition detection', 'coxian switching hidden semi-markov model', 'unified and coherent probabilistic framework', 'natural hierarchical organization of videos', 'probabilistic framework']",197,20
188,192,Most existing web video search engines index videos by file names URLs and surrounding texts. These types of video metadata roughly describe the whole video in an abstract level without taking the rich content such as semantic content descriptions and speech within the video into consideration. Therefore the relevance ranking of the video search results is not satisfactory as the details of video contents are ignored. In this paper we propose a novel relevance ranking approach for Web-based video search using both video metadata and the rich content contained in the videos. To leverage real content into ranking the videos are segmented into shots which are smaller and more semantic-meaningful retrievable units and then more detailed information of video content such as semantic descriptions and speech of each shots are used to improve the retrieval and ranking performance. With video metadata and content information of shots we developed an integrated ranking approach which achieves improved ranking performance. We also introduce machine learning into the ranking system and compare them with IR­model information retrieval model based method. The evaluation results demonstrate the effectiveness of the proposed ranking methods.,"['learning based ranking', 'video retrieval', 'IR model based ranking', 'content information', 'machine learning model', 'video search', 'metadata', 'content-based relevance ranking', 'content-based approach', 'neutral network based ranking', 'video index, relevance ranking', 'integrated ranking', 'Relevance ranking', 'IR-model', 'video metadata', 'segmented', 'Video search', 'ranking method', 'video segmentation', 'Content-based ranking']",187,20
189,193,The growing importance of access control has led to the definition of numerous languages for specifying policies. Since these languages are based on different foundations language users and designers would benefit from formal means to compare them. We present a set of properties that examine the behavior of policies under enlarged requests policy growth and policy decomposition. They therefore suggest whether policies written in these languages are easier or harder to reason about under various circumstances. We then evaluate multiple policy languages including XACML and Lithium using these properties.,"['formalize', 'properties', 'Access control', 'security', 'common features', 'policy', 'policy language property', 'access-control policy', 'multiple policy language', 'policy languague', 'xacml', 'comtemporary policy', 'access control', 'policy combinator', 'reasonability property', 'XACML', 'lithium', 'policy decomposition', 'first order logic', 'modularity', 'policy language']",89,21
190,194,In a wide range of business areas dealing with text data streams including CRM knowledge management and Web monitoring services it is an important issue to discover topic trends and analyze their dynamics in real-time.Specifically we consider the following three tasks in topic trend analysis 1)Topic Structure Identification identifying what kinds of main topics exist and how important they are 2)Topic Emergence Detection detecting the emergence of a new topic and recognizing how it grows 3)Topic Characterization  identifying the characteristics for each of main topics. For real topic analysis systems we may require that these three tasks be performed in an on-line fashion rather than in a retrospective way and be dealt with in a single framework. This paper proposes a new topic analysis framework which satisfies this requirement from a unifying viewpoint that a topic structure is modeled using a finite mixture model and that any change of a topic trend is tracked by learning the finite mixture model dynamically.In this framework we propose the usage of a time-stamp based discounting learning algorithm in order to realize real-time topic structure identification .This enables tracking the topic structure adaptively by forgetting out-of-date statistics.Further we apply the theory of dynamic model selection to detecting changes of main components in the finite mixture model in order to realize topic emergence detection.We demonstrate the effectiveness of our framework using real data collected at a help desk to show that we are able to track dynamics of topic trends in a timely fashion.,"['Data Mining', 'topic characterization', 'time-stamp based discounting learning algorithm', 'finite mixture model', 'Topic Emergence Detection', 'topic structure identification', 'text data streams', 'topic emergence detection', 'topic trends', 'Topic Characterization', 'dynamic model selection', 'time-stamp based learning algorithm', 'topic trend', 'topic analysis', 'CRM', 'text mining', 'topic detection and tracking', 'Topic Structure Identification', 'model selection', 'information gain', 'tracking dynamics']",249,21
191,195,A transactional agent is a mobile agent which manipulates objects in multiple computers by autonomously finding a way to visit the computers. The transactional agent commits only if its commitment condition like atomicity is satisfied in presence of faults of computers. On leaving a computer  an agent creates a surrogate agent which holds objects manipulated. A surrogate can recreate a new incarnation of the agent if the agent itself is faulty. If a destination computer is faulty the transactional agent finds another operational computer to visit. After visiting computers a transactional agent makes a destination on commitment according to its commitment condition. We discuss design and implementation of the transactional agent which is tolerant of computer faults.,"['transactional agent', 'Transaction', 'surrogate agent', 'Fault-Tolerant', 'mobile agent', 'ACID transaction', 'fault-tolerant', 'fault-tolerant agent', 'transaction processing', 'computer fault', 'Mobile agent']",117,11
192,197,Users cross-lingual queries to a digital library system might be short and not included in a common translation dictionary unknown terms). In this paper we investigate the feasibility of exploiting the Web as the corpus source to translate unknown query terms for cross-language information retrieval CLIR in digital libraries. We propose a Web-based term translation approach to determine effective translations for unknown query terms by mining bilingual search-result pages obtained from a real Web search engine. This approach can enhance the construction of a domain-specific bilingual lexicon and benefit CLIR services in a digital library that only has monolingual document collections. Very promising results have been obtained in generating effective translation equivalents for many unknown terms including proper nouns technical terms and Web query terms.,"['Term Translation', 'Information Search and Retrieval', 'Cross-Language Information Retrieval', 'Context Vector Analysis', 'Term Extraction', 'translation dictionary', 'Web-based term translation approach', 'Web Mining', 'CLIR services', 'BILINGUAL LEXICON CONSTRUCTION', 'PAT-Tree Based Local Maxima Algorithm', 'Unknown Cross-Lingual Queries', 'Digital Library', 'Digital Libraries']",125,14
193,198,A type-indexed function is a function that is defined for each member of some family of types. Haskell's type class mechanism provides collections of open type-indexed functions in which the indexing family can be extended by defining a new type class instance but the collection of functions is fixed. The purpose of this paper is to present TypeCase a design pattern that allows the definition of closed type-indexed functions in which the index family is fixed but the collection of functions is extensible. It is inspired by Cheney and Hinze's work on lightweight approaches to generic programming. We generalise their techniques as a design pattern . Furthermore we show that type-indexed functions with type-indexed types and consequently generic functions with generic types can also be encoded in a lightweight manner thereby overcoming one of the main limitations of the lightweight approaches.,"['Generic programming', 'type-indexed functions', 'type classes']",141,3
194,199,The research field of Intelligent Service Robots which has become more and more popular over the last years covers a wide range of applications from climbing machines for cleaning large storefronts to robotic assistance for disabled or elderly people. When developing service robot software it is a challenging problem to design the robot architecture by carefully considering user needs and requirements implement robot application components based on the architecture and integrate these components in a systematic and comprehensive way for maintainability and reusability. Furthermore it becomes more difficult to communicate among development teams and with others when many engineers from different teams participate in developing the service robot. To solve these problems we applied the COMET design method which uses the industry-standard UML notation to developing the software of an intelligent service robot for the elderly called T-Rot under development at Center for Intelligent Robotics CIR). In this paper we discuss our experiences with the project in which we successfully addressed these problems and developed the autonomous navigation system of the robot with the COMET/UML method.,"['Software engineering', 'object-oriented analysis and design methods', 'UML', 'service robot development']",176,4
195,200,This paper presents a unified utility framework for resource selection of distributed text information retrieval. This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases. With the estimated relevance information resource selection can be made by explicitly optimizing the goals of different applications. Specifically when used for database recommendation the selection is optimized for the goal of high-recall include as many relevant documents as possible in the selected databases when used for distributed document retrieval the selection targets the high-precision goal high precision in the final merged list of documents). This new model provides a more solid framework for distributed information retrieval. Empirical studies show that it is at least as effective as other state-of-the-art algorithms.,"['resource selection', 'distributed information retrieval']",129,2
196,201,The presence of unwanted or background traffic in the Internet is a well-known fact. In principle any network that has been engineered without taking its presence into account might experience troubles during periods of massive exposure to unwanted traffic e.g. during large-scale infections. A concrete example was provided by the spreading of Code-Red-II in 2001 which caused several routers crashes worldwide. Similar events might take place in 3G networks as well with further potential complications arising from their high functional complexity and the scarcity of radio resources. For example under certain hypothetical network configuration settings unwanted traffic and specifically scanning traffic from infected Mobile Stations can cause large-scale wastage of logical resources and in extreme cases even starvation. Unwanted traffic is present nowdays also in GPRS/UMTS mainly due to the widespread use of 3G connect cards for laptops. We urge the research community and network operators to consider the issue of 3G robustness to unwanted traffic as a prominent research area.,"['Cellular networks', 'Unwanted traffic', '3G']",161,3
197,202,The tools used to search and find Learning Objects in different systems do not provide a meaningful and scalable way to rank or recommend learning material. This work propose and detail the use of Contextual Attention Metadata gathered from the different tools used in the lifecycle of the Learning Object to create ranking and recommending metrics to improve the user experience. Four types of metrics are detailed Link Analysis Ranking Similarity Recommendation Personalized Ranking and Contextual Recommendation. While designed for Learning Objects it is shown that these metrics could also be applied to rank and recommend other types of reusable components like software libraries.,"['Learning Objects', 'Attention Metadata', 'Ranking', 'Recommending']",104,4
198,203,Software systems evolve over time due to changes in requirements optimization of code fixes for security and reliability bugs etc. Code churn which measures the changes made to a component over a period of time quantifies the extent of this change. We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn. Using statistical regression models we show that while absolute measures of code churn are poor predictors of defect density our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.,"['defect density', 'fault-proneness', 'Relative code churn', 'principal component analysis', 'multiple regression']",158,5
199,204,With the underlying W-CDMA technique in 3G networks resource management is a very significant issue as it can directly influence the system capacity and also lead to system QoS. However the resource can be dynamically managed in order to maintain the QoS according to the SLA. In this paper CBR is used as part of an intelligent-based agent management system. It uses information from previously managed situations to maintain the QoS in order to meet the SLA. The results illustrate the performance of an agent in traffic pattern recognition in order to identify the specific type of problem and finally propose the right solution.,"['Intelligent agent and Case-based reasoning', 'Service Level Agreement', '3G Resource Management']",104,3
200,205,Business process modeling focus on describing how activities interact with other business objects while sustaining the organization's strategy. Business objects are object-oriented representations of organizational concepts such as resources and actors which collaborate with one another in order to achieve business goals. These objects exhibit different behavior according to each specific collaboration context. This means the perception of a business object depends on its collaborations with other objects. Business process modeling techniques do not clearly separate the multiple collaborative aspects of a business object from its internal aspects making it difficult to understand objects which are used in different contexts thus hindering reuse. To cope with such issues this paper proposes using role modeling as a separation of concerns mechanism to increase the understandability and reusability of business process models. The approach divides a business process model into a business object model and a role model. The business object models deals with specifying the structure and intrinsic behavior of business objects while the role model specifies its collaborative aspects.,"['Business Object', 'Organizational Engineering', 'Business Process Modeling', 'Role Modeling']",169,4
201,206,Personalized information agents can help overcome some of the limitations of communal Web information sources such as portals and search engines. Two important components of these agents are user profiles and information filtering or gathering services. Ideally these components can be sep-arated so that a single user profile can be leveraged for a variety of information services. Toward that end we are building an information agent called SurfAgent;in previous studies we have developed and tested methods for automatically learning a user profile 20]. In this paper we evaluate alternative methods for recommending new documents to a user by generating queries from the user profile and submitting them to a popular search engine. Our study focuses on three questions How do different algorithms for query generation perform relative to each other Is positive relevance feedback adequate to support the task Can a user profile be learned independent of the service We found that three algorithms appear to excel and that using only positive feedback does degrade the results somewhat. We conclude with the results of a pilot user study for assessing interaction of the profile and the query generation mechanisms.,"['query generation', 'information agents', 'user modeling']",189,3
202,207,This paper presents a novel macroblock mode decision algorithm for inter-frame prediction based on machine learning techniques to be used as part of a very low complexity MPEG-2 to H.264 video transcoder. Since coding mode decisions take up the most resources in video transcoding a fast macro block MB mode estimation would lead to reduced complexity. The proposed approach is based on the hypothesis that MB coding mode decisions in H.264 video have a correlation with the distribution of the motion compensated residual in MPEG-2 video. We use machine learning tools to exploit the correlation and derive decision trees to classify the incoming MPEG-2 MBs into one of the 11 coding modes in H.264. The proposed approach reduces the H.264 MB mode computation process into a decision tree lookup with very low complexity. The proposed transcoder is compared with a reference transcoder comprised of a MPEG-2 decoder and an H.264 encoder. Our results show that the proposed transcoder reduces the H.264 encoding time by over 95 with negligible loss in quality and bitrate.,"['Machine Learning', 'Inter-frame', 'MPEG-2', 'H.264', 'Transcoding']",173,5
203,208,The emergence of third-generation  3G mobile networks offers new opportunities for the effective delivery of data with rich content including multimedia messaging and video-streaming. Provided that streaming services have proved highly successful over stationary networks in the past we anticipate that the same trend will soon take place in 3G networks. Although mobile operators currently make available pertinent services the available resources of the underlying networks for the delivery of rich data remain in-herently constrained. At this stage and in light of large numbers of users moving fast across cells 3G networks may not be able to warrant the needed quality-of-service requirements. The support for streaming services necessitates the presence of content or media servers properly placed over the 3G network such servers essen-tially become the source for streaming applications. Evidently a centralized approach in organizing streaming content might lead to highly congested media-nodes which in presence of moving users will certainly yield increased response times and jitter to user requests . In this paper we propose a workaround that enables 3G networks to offer uninterrupted video-streaming services in the presence of a large number of users moving in high-speed. At the same time we offer a distributed organization for the network's media-servers to better handle over-utilization.,"['rate adaptation', 'Streaming for moving users', 'mobile multimedia services', 'real-time streaming']",207,4
204,209,We address the problem of integrating objects from a source taxonomy into a master taxonomy. This problem is not only currently pervasive on the web but also important to the emerging semantic web. A straightforward approach to automating this process would be to learn a classifier that can classify objects from the source taxonomy into categories of the master taxonomy. The key insight is that the availability of the source taxonomy data could be helpful to build better classifiers for the master taxonomy if their categorizations have some semantic overlap. In this paper we propose a new approach co-bootstrapping  to enhance the classification by exploiting such implicit knowledge. Our experiments with real-world web data show substantial improvements in the performance of taxonomy integration.,"['Machine Learning', 'Boosting', 'Ontology Mapping', 'Taxonomy Integration', 'Semantic Web', 'Classification', 'Bootstrapping']",123,7
205,210,Today web search engines provide the easiest way to reach information on the web. In this scenario more than 95 of Indian language content on the web is not searchable due to multiple encodings of web pages. Most of these encodings are proprietary and hence need some kind of standardization for making the content accessible via a search engine. In this paper we present a search engine called WebKhoj which is capable of searching multi-script and multi-encoded Indian language content on the web. We describe a language focused crawler and the transcoding processes involved to achieve accessibility of Indian langauge content. In the end we report some of the experiments that were conducted along with results on Indian language web content.,"['non-standard encodings', 'web search', 'Indian languages']",121,3
206,211,Some large scale topical digital libraries such as CiteSeer harvest online academic documents by crawling open-access archives university and author homepages and authors self-submissions. While these approaches have so far built reasonable size libraries they can suffer from having only a portion of the documents from specific publishing venues. We propose to use alternative online resources and techniques that maximally exploit other resources to build the complete document collection of any given publication venue. We investigate the feasibility of using publication metadata to guide the crawler towards authors homepages to harvest what is missing from a digital library collection. We collect a real-world dataset from two Computer Science publishing venues involving a total of 593 unique authors over a time frame of 1998 to 2004. We then identify the missing papers that are not indexed by CiteSeer. Using a fully automatic heuristic-based system that has the capability of locating authors homepages and then using focused crawling to download the desired papers we demonstrate that it is practical to harvest using a focused crawler academic papers that are missing from our digital library. Our harvester achieves a performance with an average recall level of 0.82 overall and 0.75 for those missing documents. Evaluation of the crawler's performance based on the harvest rate shows definite advantages over other crawling approaches and consistently outperforms a defined baseline crawler on a number of measures.,"['CiteSeer', 'Digital libraries', 'focused crawler', 'DBLP', 'harvesting', 'ACM']",230,6
